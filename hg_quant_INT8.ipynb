{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Whisper Model INT8 Quantization and Deployment\n",
        "\n",
        "This notebook demonstrates the process of optimizing a Whisper speech recognition model using INT8 quantization with NVIDIA TensorRT-LLM for efficient inference. The workflow consists of several key stages:\n",
        "\n",
        "## 1. Environment Setup\n",
        "We first set up a Python 3.10 environment, which is required for compatibility with TensorRT-LLM. The initial cells install the necessary dependencies and clone the TensorRT-LLM repository from NVIDIA, which provides tools for optimizing large language models.\n",
        "\n",
        "## 2. Model Preparation\n",
        "We download the necessary assets for the Whisper model:\n",
        "- The multilingual tokenizer (used for converting between text and tokens)\n",
        "- Mel filters (used for processing audio spectrograms)\n",
        "- A sample audio file for testing\n",
        "\n",
        "Then we convert a specialized Whisper model (\"jharshraj/whisper-indian-names\") which has been fine-tuned for better recognition of Indian names. This model is downloaded and prepared for the quantization process.\n",
        "\n",
        "## 3. INT8 Quantization\n",
        "INT8 quantization is a model optimization technique that reduces the precision of the model weights from 32-bit floating point to 8-bit integers. This process:\n",
        "- Significantly reduces the model size (approximately 4x smaller)\n",
        "- Improves inference speed\n",
        "- Reduces memory requirements\n",
        "- Maintains reasonable accuracy for speech recognition tasks\n",
        "\n",
        "We use weight-only quantization, which quantizes only the weights while keeping activations in FP16 format, achieving a good balance between performance and accuracy.\n",
        "\n",
        "## 4. TensorRT Engine Building\n",
        "After quantization, we build TensorRT engines for both the encoder and decoder components of the Whisper model:\n",
        "- The encoder processes the audio features into a meaningful representation\n",
        "- The decoder generates text from the encoded representation\n",
        "\n",
        "These engines are optimized for GPU execution with specific parameters like batch size, beam width, and sequence length tailored for speech-to-text applications.\n",
        "\n",
        "## 5. Inference Testing\n",
        "We test the optimized model with a sample audio file to verify that the quantized model works correctly. This helps ensure that our optimization hasn't significantly degraded recognition quality.\n",
        "\n",
        "## 6. Triton Server Deployment Preparation\n",
        "Finally, we prepare the model for deployment on NVIDI"
      ],
      "metadata": {
        "id": "O82LaiOrRTXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup\n",
        "We first set up a Python 3.10 environment, which is required for compatibility with TensorRT-LLM. The initial cells install the necessary dependencies and clone the TensorRT-LLM repository from NVIDIA, which provides tools for optimizing large language models."
      ],
      "metadata": {
        "id": "ePCRivvVSAjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Environment Setup\n"
      ],
      "metadata": {
        "id": "U8CiKFmISKsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Python 3.10\n",
        "!apt-get update -y\n",
        "!apt-get install -y python3.10 python3.10-dev python3.10-distutils\n",
        "\n",
        "# Install pip for Python 3.10\n",
        "!wget https://bootstrap.pypa.io/get-pip.py\n",
        "!python3.10 get-pip.py\n",
        "\n",
        "# Make Python 3.10 the default python interpreter\n",
        "!ln -sf /usr/bin/python3.10 /usr/local/bin/python\n",
        "!hash -r\n",
        "\n",
        "# Verify the Python version\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRO3acNs1WsD",
        "outputId": "b6b6499c-c802-4c16-b1f1-2b5bfeb9b064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,762 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,677 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,000 kB]\n",
            "Fetched 20.3 MB in 2s (10.9 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'python3-distutils' instead of 'python3.10-distutils'\n",
            "python3-distutils is already the newest version (3.10.8-1~22.04).\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.9).\n",
            "python3.10 set to manually installed.\n",
            "python3.10-dev is already the newest version (3.10.12-1~22.04.9).\n",
            "python3.10-dev set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "--2025-03-20 22:40:38--  https://bootstrap.pypa.io/get-pip.py\n",
            "Resolving bootstrap.pypa.io (bootstrap.pypa.io)... 151.101.0.175, 151.101.64.175, 151.101.128.175, ...\n",
            "Connecting to bootstrap.pypa.io (bootstrap.pypa.io)|151.101.0.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2300175 (2.2M) [text/x-python]\n",
            "Saving to: ‘get-pip.py’\n",
            "\n",
            "get-pip.py          100%[===================>]   2.19M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-03-20 22:40:38 (23.8 MB/s) - ‘get-pip.py’ saved [2300175/2300175]\n",
            "\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-77.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-77.0.3-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "Successfully installed pip-25.0.1 setuptools-77.0.3 wheel-0.45.1\n",
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98kQuAsmDwwC",
        "outputId": "12a5f41d-7f77-4627-d1e4-52c669604a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "## 2. TensorRT-LLM Repository Setup\n",
        "Here we clone NVIDIA's TensorRT-LLM repository which provides the necessary tools for optimizing and deploying LLM models including Whisper. We also provide code for extracting files if you've downloaded the repository as a zip.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "zCqBrUThSV80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorRT-LLM Repository Setup\n",
        "!git clone https://github.com/NVIDIA/TensorRT-LLM.git"
      ],
      "metadata": {
        "id": "DIVcEVwpSn0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/TensorRT-LLM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXlM0dfk76VP",
        "outputId": "d60da4ce-85ea-4b04-c2eb-7178d0776cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TensorRT-LLM'...\n",
            "remote: Enumerating objects: 46658, done.\u001b[K\n",
            "remote: Counting objects: 100% (206/206), done.\u001b[K\n",
            "remote: Compressing objects: 100% (128/128), done.\u001b[K\n",
            "remote: Total 46658 (delta 141), reused 78 (delta 78), pack-reused 46452 (from 4)\u001b[K\n",
            "Receiving objects: 100% (46658/46658), 929.85 MiB | 17.55 MiB/s, done.\n",
            "Resolving deltas: 100% (34605/34605), done.\n",
            "Updating files: 100% (6113/6113), done.\n",
            "Filtering content: 100% (6/6), 675.44 MiB | 66.49 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you already have downloaed a zip file\n",
        "\n",
        "# Import required libraries\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define file paths\n",
        "zip_path = '/content/your_file.zip'  # Path to your zip file\n",
        "extract_path = '/content/'  # Path where you want to extract files\n",
        "\n",
        "# Create extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")\n",
        "\n",
        "# List extracted files (optional)\n",
        "print(\"\\nExtracted files:\")\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    for file in files:\n",
        "        print(os.path.join(root, file))\n"
      ],
      "metadata": {
        "id": "hXaHcdewuGen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "## 3. Dependency Installation\n",
        "Installing required packages for the Whisper example and downloading necessary assets including tokenizer and mel filters needed for audio processing.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-ijkmdqLSv9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Dependencies and Downloading Assets"
      ],
      "metadata": {
        "id": "vnHPlcr0S2e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/TensorRT-LLM/examples/whisper/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W1dnsZq96tE",
        "outputId": "15e398a9-5671-4e6f-a212-2fb82fa5f6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorrt_llm>=0.0.0.dev0 in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.19.0.dev2025031800)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 3)) (0.9.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: kaldialign in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 5)) (0.9.1)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 6)) (20240930)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 8)) (0.13.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 9)) (0.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (4.48.3)\n",
            "Requirement already satisfied: janus in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: accelerate>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.2.2.post1)\n",
            "Requirement already satisfied: colored in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: cuda-python in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.8.0)\n",
            "Requirement already satisfied: diffusers>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.32.2)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.2.2)\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: onnx-graphsurgeon>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.5.6)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.67.0)\n",
            "Requirement already satisfied: polygraphy in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.49.20)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (7.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py>=12 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.570.86)\n",
            "Requirement already satisfied: pynvml>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.0.0)\n",
            "Requirement already satisfied: pulp in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: h5py==3.12.1 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (3.12.1)\n",
            "Requirement already satisfied: StrEnum in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.4.15)\n",
            "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: tensorrt~=10.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (10.8.0.43)\n",
            "Requirement already satisfied: torch<=2.6.0,>=2.6.0a0 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.21.0)\n",
            "Requirement already satisfied: nvidia-modelopt~=0.25.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt[torch]~=0.25.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.25.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: pydantic>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.10.6)\n",
            "Requirement already satisfied: pillow==10.3.0 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (10.3.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.45.1)\n",
            "Requirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.24.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.4.3)\n",
            "Requirement already satisfied: mpmath>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (8.1.8)\n",
            "Requirement already satisfied: click-option-group in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.5.6)\n",
            "Requirement already satisfied: aenum in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (3.1.15)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (26.3.0)\n",
            "Requirement already satisfied: fastapi==0.115.4 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.115.4)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.34.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.28.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (77.0.3)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (4.1.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.15.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: flashinfer-python~=0.2.3 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.2.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (4.11.0.86)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.115.4->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.41.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.115.4->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (3.11.14)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 6)) (0.61.0)\n",
            "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from openai-whisper->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 6)) (8.10.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (5.2.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 8)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 8)) (2.22)\n",
            "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers>=0.27.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (4.6.4)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flashinfer-python~=0.2.3->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.11.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 4)) (1.18.3)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 6)) (0.44.0)\n",
            "Requirement already satisfied: nvidia-modelopt-core==0.25.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt~=0.25.0->nvidia-modelopt[torch]~=0.25.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.25.0)\n",
            "Requirement already satisfied: cloudpickle>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt~=0.25.0->nvidia-modelopt[torch]~=0.25.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt~=0.25.0->nvidia-modelopt[torch]~=0.25.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (13.9.4)\n",
            "Requirement already satisfied: torchprofile>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from nvidia-modelopt[torch]~=0.25.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.0.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (6.30.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (4.3.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.1->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.1->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 10)) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->librosa->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: tensorrt_cu12==10.8.0.43 in /usr/local/lib/python3.10/dist-packages (from tensorrt~=10.8.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (10.8.0.43)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: cuda-bindings~=12.8.0 in /usr/local/lib/python3.10/dist-packages (from cuda-python->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (12.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2025.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch<=2.6.0,>=2.6.0a0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->nvidia-modelopt~=0.25.0->nvidia-modelopt[torch]~=0.25.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->nvidia-modelopt~=0.25.0->nvidia-modelopt[torch]~=0.25.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->nvidia-modelopt~=0.25.0->nvidia-modelopt[torch]~=0.25.0->tensorrt_llm>=0.0.0.dev0->-r /content/TensorRT-LLM/examples/whisper/requirements.txt (line 2)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaNEd78EG0y0",
        "outputId": "3b838110-9d5b-434d-cd0c-a877e5f8ffc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TensorRT-LLM/examples/whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMXBVvca-34h",
        "outputId": "f4b33503-2c7a-4a38-e596-3f93aa5a13b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TensorRT-LLM/examples/whisper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download tokenizer and mel filters\n",
        "!wget --directory-prefix=assets https://raw.githubusercontent.com/openai/whisper/main/whisper/assets/multilingual.tiktoken\n",
        "!wget --directory-prefix=assets https://raw.githubusercontent.com/openai/whisper/main/whisper/assets/mel_filters.npz\n",
        "\n",
        "# Download a sample audio file for testing (optional)\n",
        "!wget --directory-prefix=assets https://raw.githubusercontent.com/yuekaizhang/Triton-ASR-Client/main/datasets/mini_en/wav/1221-135766-0002.wav"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6r3uFt5GY5a",
        "outputId": "a4b8fdf7-931e-4c34-807e-a22d2328c754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-20 19:48:50--  https://raw.githubusercontent.com/openai/whisper/main/whisper/assets/multilingual.tiktoken\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 816730 (798K) [text/plain]\n",
            "Saving to: ‘assets/multilingual.tiktoken’\n",
            "\n",
            "multilingual.tiktok 100%[===================>] 797.59K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2025-03-20 19:48:50 (163 MB/s) - ‘assets/multilingual.tiktoken’ saved [816730/816730]\n",
            "\n",
            "--2025-03-20 19:48:50--  https://raw.githubusercontent.com/openai/whisper/main/whisper/assets/mel_filters.npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4271 (4.2K) [application/octet-stream]\n",
            "Saving to: ‘assets/mel_filters.npz’\n",
            "\n",
            "mel_filters.npz     100%[===================>]   4.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-20 19:48:51 (58.0 MB/s) - ‘assets/mel_filters.npz’ saved [4271/4271]\n",
            "\n",
            "--2025-03-20 19:48:51--  https://raw.githubusercontent.com/yuekaizhang/Triton-ASR-Client/main/datasets/mini_en/wav/1221-135766-0002.wav\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154444 (151K) [audio/wav]\n",
            "Saving to: ‘assets/1221-135766-0002.wav’\n",
            "\n",
            "1221-135766-0002.wa 100%[===================>] 150.82K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2025-03-20 19:48:51 (40.3 MB/s) - ‘assets/1221-135766-0002.wav’ saved [154444/154444]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "## 4. Model Conversion\n",
        "Converting a specialized Whisper model fine-tuned for Indian names recognition to a format compatible with TensorRT-LLM. This step downloads the model from Hugging Face and prepares it for quantization.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "D2HScnSpS8er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Model Conversion from Hugging Face"
      ],
      "metadata": {
        "id": "nTCeDcqNS_2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python distil_whisper/convert_from_distil_whisper.py \\\n",
        "  --model_name \"jharshraj/whisper-indian-names\" \\\n",
        "  --output_dir \"./assets/\" \\\n",
        "  --output_name \"my_whisper_model\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SINC0m4A-TOS",
        "outputId": "c2ca1545-a929-44e5-aeba-f6a75618a4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the model:\n",
            "100% 479/479 [00:00<00:00, 40678.52it/s]\n",
            "Param keys have been changed. Saving the model...\n",
            "Directory './assets/' created successfully!\n",
            "Model saved to  assets/my_whisper_model.pt\n",
            "Kindly use that to build the tensorrt_llm engine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAubO---El7B",
        "outputId": "8e88c4a6-1708-4529-d701-85c4a39c1e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorrt                 10.8.0.43\n",
            "tensorrt_cu12            10.8.0.43\n",
            "tensorrt_cu12_bindings   10.8.0.43\n",
            "tensorrt_cu12_libs       10.8.0.43\n",
            "tensorrt-llm             0.19.0.dev2025031800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "## 5. INT8 Quantization\n",
        "Performing INT8 weight-only quantization on the model. This process reduces the model's precision from FP32/FP16 to INT8 for the weights, significantly reducing memory requirements while maintaining reasonable accuracy.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3O2WPofCTGjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INT8 Quantization"
      ],
      "metadata": {
        "id": "R9cGE1sPTLBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TensorRT-LLM/examples/whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZA1FwtTEtI_",
        "outputId": "4ea328f0-a6f3-4ffb-acf8-3e1aab775be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TensorRT-LLM/examples/whisper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the current directory\n",
        "import os\n",
        "current_dir = os.getcwd()\n",
        "print(f\"Current directory: {current_dir}\")\n",
        "\n",
        "# Run the conversion script\n",
        "!python {current_dir}/convert_checkpoint.py \\\n",
        "    --use_weight_only \\\n",
        "    --weight_only_precision int8 \\\n",
        "    --output_dir whisper_model_weights_int8 \\\n",
        "    --model_dir assets \\\n",
        "    --model_name my_whisper_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGUaCRDUAQkO",
        "outputId": "deda1f48-5b6d-4e1f-f486-fa6d1726ad5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content/TensorRT-LLM/examples/whisper\n",
            "2025-03-20 19:45:21,061 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "[TensorRT-LLM] TensorRT-LLM version: 0.19.0.dev2025031800\n",
            "0.19.0.dev2025031800\n",
            "Loaded model from assets/my_whisper_model.pt\n",
            "Converting encoder checkpoints...\n",
            "Converting decoder checkpoints...\n",
            "Total time of converting checkpoints: 00:00:04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"\"\"\n",
        "## 6. TensorRT Engine Building\n",
        "Building optimized TensorRT engines for both the encoder and decoder components of the Whisper model. These engines are highly optimized for GPU inference.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_jsuI5wYTUyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorRT Engine Building"
      ],
      "metadata": {
        "id": "n_L7iIz0TYAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Set up variables\n",
        "INFERENCE_PRECISION=float16\n",
        "WEIGHT_ONLY_PRECISION=int8\n",
        "MAX_BEAM_WIDTH=4\n",
        "MAX_BATCH_SIZE=8\n",
        "MODEL_NAME=my_whisper_model\n",
        "checkpoint_dir=whisper_model_weights_${WEIGHT_ONLY_PRECISION}\n",
        "output_dir=whisper_model_${WEIGHT_ONLY_PRECISION}\n",
        "\n",
        "# Build encoder engine\n",
        "trtllm-build  --checkpoint_dir ${checkpoint_dir}/encoder \\\n",
        "              --output_dir ${output_dir}/encoder \\\n",
        "              --moe_plugin disable \\\n",
        "              --max_batch_size ${MAX_BATCH_SIZE} \\\n",
        "              --gemm_plugin disable \\\n",
        "              --bert_attention_plugin ${INFERENCE_PRECISION} \\\n",
        "              --max_input_len 3000 --max_seq_len=3000\n",
        "\n",
        "# Build decoder engine\n",
        "trtllm-build  --checkpoint_dir ${checkpoint_dir}/decoder \\\n",
        "              --output_dir ${output_dir}/decoder \\\n",
        "              --moe_plugin disable \\\n",
        "              --max_beam_width ${MAX_BEAM_WIDTH} \\\n",
        "              --max_batch_size ${MAX_BATCH_SIZE} \\\n",
        "              --max_seq_len 114 \\\n",
        "              --max_input_len 14 \\\n",
        "              --max_encoder_input_len 3000 \\\n",
        "              --gemm_plugin ${INFERENCE_PRECISION} \\\n",
        "              --bert_attention_plugin ${INFERENCE_PRECISION} \\\n",
        "              --gpt_attention_plugin ${INFERENCE_PRECISION}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXYSDY4XB7K9",
        "outputId": "397f8860-c2bc-4f8f-ea23-9a535e9496e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TensorRT-LLM] TensorRT-LLM version: 0.19.0.dev2025031800\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set gpt_attention_plugin to auto.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set gemm_plugin to None.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set nccl_plugin to auto.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set lora_plugin to None.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set dora_plugin to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set moe_plugin to None.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set low_latency_gemm_plugin to None.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set low_latency_gemm_swiglu_plugin to None.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set gemm_allreduce_plugin to None.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set context_fmha to True.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set remove_input_padding to True.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set norm_quant_fusion to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set reduce_fusion to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set user_buffer to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set tokens_per_block to 32.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set use_paged_context_fmha to True.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set use_fp8_context_fmha to True.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set fuse_fp4_quant to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set multiple_profiles to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set paged_state to True.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set streamingllm to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set use_fused_mlp to True.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set pp_reduce_scatter to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_position_embedding = True\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [W] Implicitly setting PretrainedConfig.n_mels = 80\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [W] Implicitly setting PretrainedConfig.num_languages = 99\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [W] Parameter was initialized as DataType.HALF but set to DataType.FLOAT\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set dtype to float16.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [W] Overriding paged_state to False\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set paged_state to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [W] max_seq_len 3000 is larger than max_position_embeddings 1500 * rotary scaling 1, the model accuracy might be affected\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
            "\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
            "[03/20/2025-19:46:10] [TRT-LLM] [W] FP8 Context FMHA is disabled because it must be used together with the fp8 quantization workflow.\n",
            "[03/20/2025-19:46:10] [TRT] [I] [MemUsageChange] Init CUDA: CPU -14, GPU +0, now: CPU 298, GPU 189 (MiB)\n",
            "[03/20/2025-19:46:14] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +2769, GPU +446, now: CPU 3173, GPU 635 (MiB)\n",
            "[03/20/2025-19:46:14] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to float16.\n",
            "[03/20/2025-19:46:14] [TRT-LLM] [I] Set nccl_plugin to None.\n",
            "[03/20/2025-19:46:14] [TRT-LLM] [I] Total time of constructing network from module object 4.455483913421631 seconds\n",
            "[03/20/2025-19:46:14] [TRT-LLM] [I] Total optimization profiles added: 1\n",
            "[03/20/2025-19:46:14] [TRT-LLM] [I] Total time to initialize the weights in network Unnamed Network 0: 00:00:00\n",
            "[03/20/2025-19:46:14] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n",
            "[03/20/2025-19:46:15] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
            "[03/20/2025-19:46:15] [TRT] [I] Compiler backend is used during engine build.\n",
            "[03/20/2025-19:46:18] [TRT] [I] Detected 3 inputs and 1 output network tensors.\n",
            "[03/20/2025-19:46:21] [TRT] [I] Total Host Persistent Memory: 37344 bytes\n",
            "[03/20/2025-19:46:21] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
            "[03/20/2025-19:46:21] [TRT] [I] Max Scratch Memory: 55296000 bytes\n",
            "[03/20/2025-19:46:21] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 202 steps to complete.\n",
            "[03/20/2025-19:46:21] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.39496ms to assign 5 blocks to 202 nodes requiring 184320512 bytes.\n",
            "[03/20/2025-19:46:21] [TRT] [I] Total Activation Memory: 184320512 bytes\n",
            "[03/20/2025-19:46:21] [TRT] [I] Total Weights Memory: 93926656 bytes\n",
            "[03/20/2025-19:46:21] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[03/20/2025-19:46:27] [TRT] [I] Engine generation completed in 12.1294 seconds.\n",
            "[03/20/2025-19:46:27] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 3 MiB, GPU 147 MiB\n",
            "[03/20/2025-19:46:27] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:12\n",
            "[03/20/2025-19:46:27] [TRT] [I] Serialized 27 bytes of code generator cache.\n",
            "[03/20/2025-19:46:27] [TRT] [I] Serialized 185725 bytes of compilation cache.\n",
            "[03/20/2025-19:46:27] [TRT] [I] Serialized 133 timing cache entries\n",
            "[03/20/2025-19:46:27] [TRT-LLM] [I] Timing cache serialized to model.cache\n",
            "[03/20/2025-19:46:27] [TRT-LLM] [I] Build phase peak memory: 6239.55 MB, children: 21.75 MB\n",
            "[03/20/2025-19:46:27] [TRT-LLM] [I] Serializing engine to whisper_model_int8/encoder/rank0.engine...\n",
            "[03/20/2025-19:46:27] [TRT-LLM] [I] Engine serialized. Total time: 00:00:00\n",
            "[03/20/2025-19:46:27] [TRT-LLM] [I] Total time of building all engines: 00:00:17\n",
            "[TensorRT-LLM] TensorRT-LLM version: 0.19.0.dev2025031800\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set gpt_attention_plugin to float16.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set gemm_plugin to float16.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set nccl_plugin to auto.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set lora_plugin to None.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set dora_plugin to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set moe_plugin to None.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set low_latency_gemm_plugin to None.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set low_latency_gemm_swiglu_plugin to None.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set gemm_allreduce_plugin to None.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set context_fmha to True.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set remove_input_padding to True.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set norm_quant_fusion to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set reduce_fusion to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set user_buffer to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set tokens_per_block to 32.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set use_paged_context_fmha to True.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set use_fp8_context_fmha to True.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set fuse_fp4_quant to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set multiple_profiles to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set paged_state to True.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set streamingllm to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set use_fused_mlp to True.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set pp_reduce_scatter to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.use_prompt_tuning = False\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_position_embedding = True\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.layernorm_type = 0\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_attention_qkvo_bias = True\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_mlp_bias = True\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_model_final_layernorm = True\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_embedding_layernorm = False\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_embedding_scale = False\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.ffn_hidden_size = 3072\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.q_scaling = 1.0\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.layernorm_position = 0\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.relative_attention = False\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.max_distance = 0\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.num_buckets = 0\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.model_type = whisper\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.rescale_before_lm_head = False\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.encoder_hidden_size = 768\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.encoder_num_heads = 12\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.encoder_head_size = None\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Implicitly setting PretrainedConfig.skip_cross_kv = False\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set dtype to float16.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] Overriding paged_state to False\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set paged_state to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
            "\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] max_num_tokens (8192) shouldn't be greater than max_seq_len * max_batch_size (912), specifying to max_seq_len * max_batch_size (912).\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
            "[03/20/2025-19:46:37] [TRT-LLM] [W] FP8 Context FMHA is disabled because it must be used together with the fp8 quantization workflow.\n",
            "[03/20/2025-19:46:37] [TRT] [I] [MemUsageChange] Init CUDA: CPU -13, GPU +0, now: CPU 287, GPU 189 (MiB)\n",
            "[03/20/2025-19:46:41] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +2770, GPU +446, now: CPU 3175, GPU 635 (MiB)\n",
            "[03/20/2025-19:46:41] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to float16.\n",
            "[03/20/2025-19:46:41] [TRT-LLM] [I] Set nccl_plugin to None.\n",
            "[03/20/2025-19:46:41] [TRT-LLM] [I] Total time of constructing network from module object 4.6088547706604 seconds\n",
            "[03/20/2025-19:46:41] [TRT-LLM] [I] Total optimization profiles added: 1\n",
            "[03/20/2025-19:46:41] [TRT-LLM] [I] Total time to initialize the weights in network Unnamed Network 0: 00:00:00\n",
            "[03/20/2025-19:46:41] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n",
            "[03/20/2025-19:46:41] [TRT] [W] Unused Input: host_kv_cache_block_offsets\n",
            "[03/20/2025-19:46:41] [TRT] [W] Unused Input: cross_kv_cache_gen\n",
            "[03/20/2025-19:46:41] [TRT] [W] [RemoveDeadLayers] Input Tensor host_kv_cache_block_offsets is unused or used only at compile-time, but is not being removed.\n",
            "[03/20/2025-19:46:41] [TRT] [W] [RemoveDeadLayers] Input Tensor cross_kv_cache_gen is unused or used only at compile-time, but is not being removed.\n",
            "[03/20/2025-19:46:41] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
            "[03/20/2025-19:46:41] [TRT] [I] Compiler backend is used during engine build.\n",
            "[03/20/2025-19:46:44] [TRT] [E] Error Code: 9: Skipping tactic 0x00000000000003ea due to exception Unsupported data type Bool.\n",
            "[03/20/2025-19:46:45] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
            "[03/20/2025-19:46:45] [TRT] [I] Detected 27 inputs and 1 output network tensors.\n",
            "[03/20/2025-19:46:47] [TRT] [I] Total Host Persistent Memory: 81968 bytes\n",
            "[03/20/2025-19:46:47] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
            "[03/20/2025-19:46:47] [TRT] [I] Max Scratch Memory: 33728256 bytes\n",
            "[03/20/2025-19:46:47] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 397 steps to complete.\n",
            "[03/20/2025-19:46:47] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 76.4921ms to assign 28 blocks to 397 nodes requiring 222571520 bytes.\n",
            "[03/20/2025-19:46:47] [TRT] [I] Total Activation Memory: 222570496 bytes\n",
            "[03/20/2025-19:46:47] [TRT] [I] Total Weights Memory: 274112384 bytes\n",
            "[03/20/2025-19:46:47] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[03/20/2025-19:46:49] [TRT] [I] Engine generation completed in 7.66782 seconds.\n",
            "[03/20/2025-19:46:49] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 317 MiB\n",
            "[03/20/2025-19:46:49] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:08\n",
            "[03/20/2025-19:46:49] [TRT] [I] Serialized 27 bytes of code generator cache.\n",
            "[03/20/2025-19:46:49] [TRT] [I] Serialized 144203 bytes of compilation cache.\n",
            "[03/20/2025-19:46:49] [TRT] [I] Serialized 38 timing cache entries\n",
            "[03/20/2025-19:46:49] [TRT-LLM] [I] Timing cache serialized to model.cache\n",
            "[03/20/2025-19:46:49] [TRT-LLM] [I] Build phase peak memory: 6561.24 MB, children: 21.82 MB\n",
            "[03/20/2025-19:46:49] [TRT-LLM] [I] Serializing engine to whisper_model_int8/decoder/rank0.engine...\n",
            "[03/20/2025-19:46:50] [TRT-LLM] [I] Engine serialized. Total time: 00:00:00\n",
            "[03/20/2025-19:46:50] [TRT-LLM] [I] Total time of building all engines: 00:00:13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-20 19:46:10,392 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "2025-03-20 19:46:36,986 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "## 7. Testing and Inference\n",
        "Installing additional dependencies for audio processing and running inference on a sample audio file to verify the quantized model works correctly.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GLCaxX-RTrun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing libraries specific to run the run file/inference\n",
        "!pip install transformers librosa torchaudio soundfile\n",
        "!pip install datasets\n",
        "!pip install openai-whisper\n",
        "!pip install soundfile librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK4zH8JZHaZT",
        "outputId": "63da9ba5-70c9-4ac1-d12f-018eee78b199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.48.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.61.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.2.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch==2.6.0->torchaudio) (2.0.1)\n",
            "Downloading torchaudio-2.6.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchaudio\n",
            "Successfully installed torchaudio-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 run.py \\\n",
        "  --name single_wav_test \\\n",
        "  --engine_dir whisper_model_int8 \\\n",
        "  --assets_dir assets \\\n",
        "  --input_file assets/1221-135766-0002.wav"
      ],
      "metadata": {
        "id": "i32NMJWiCrFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70ebfb9-4488-470a-ec6c-c3d4adc7aea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-20 19:58:53,074 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
            "[TensorRT-LLM] TensorRT-LLM version: 0.19.0.dev2025031800\n",
            "[TensorRT-LLM][INFO] Engine version 0.19.0.dev2025031800 found in the config file, assuming engine(s) built by new builder API.\n",
            "[TensorRT-LLM][INFO] Setting encoder max input length and hidden size for accepting visual features.\n",
            "[TensorRT-LLM][INFO] Engine version 0.19.0.dev2025031800 found in the config file, assuming engine(s) built by new builder API.\n",
            "[TensorRT-LLM][INFO] Engine version 0.19.0.dev2025031800 found in the config file, assuming engine(s) built by new builder API.\n",
            "[TensorRT-LLM][INFO] Setting encoder max input length and hidden size for accepting visual features.\n",
            "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.use_prompt_tuning = False\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_position_embedding = True\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.layernorm_type = 0\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_attention_qkvo_bias = True\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_mlp_bias = True\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_model_final_layernorm = True\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_embedding_layernorm = False\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_embedding_scale = False\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.ffn_hidden_size = 3072\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.q_scaling = 1.0\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.layernorm_position = 0\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.relative_attention = False\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.max_distance = 0\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.num_buckets = 0\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.model_type = whisper\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.rescale_before_lm_head = False\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.encoder_hidden_size = 768\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.encoder_num_heads = 12\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.encoder_head_size = None\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.skip_cross_kv = False\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.type_vocab_size = None\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.encoder_num_kv_heads = None\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.mlp_type = 0\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.residual_scaling = 1.0\n",
            "[03/20/2025-19:58:53] [TRT-LLM] [W] Implicitly setting PretrainedConfig.has_lm_head_bias = False\n",
            "[TensorRT-LLM][INFO] Engine version 0.19.0.dev2025031800 found in the config file, assuming engine(s) built by new builder API.\n",
            "[TensorRT-LLM][INFO] Setting encoder max input length and hidden size for accepting visual features.\n",
            "[TensorRT-LLM][INFO] Refreshed the MPI local session\n",
            "[TensorRT-LLM][INFO] Engine version 0.19.0.dev2025031800 found in the config file, assuming engine(s) built by new builder API.\n",
            "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 8\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 8\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 3000\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
            "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (3000) * 12\n",
            "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
            "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 8192\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 2999  = maxSequenceLen - 1 since chunked context is enabled\n",
            "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: 3000 = maxSequenceLen.\n",
            "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
            "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
            "[TensorRT-LLM][INFO] Loaded engine size: 92 MiB\n",
            "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
            "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
            "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 175.78 MiB for execution context memory.\n",
            "[TensorRT-LLM][INFO] [MS] Running engine with multi stream info\n",
            "[TensorRT-LLM][INFO] [MS] Number of aux streams is 1\n",
            "[TensorRT-LLM][INFO] [MS] Number of total worker streams is 2\n",
            "[TensorRT-LLM][INFO] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
            "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 89 (MiB)\n",
            "[TensorRT-LLM][INFO] TRTEncoderModel mMaxInputLen: reset to 3000 from build config.\n",
            "[TensorRT-LLM][INFO] MPI size: 1, MPI local size: 1, rank: 0\n",
            "[TensorRT-LLM][INFO] Rank 0 is using GPU 0\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 4\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 4\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxBeamWidth: 1\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxSequenceLen: 114\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxDraftLen: 0\n",
            "[TensorRT-LLM][INFO] TRTGptModel mMaxAttentionWindowSize: (114) * 12\n",
            "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 0\n",
            "[TensorRT-LLM][INFO] TRTGptModel normalizeLogProbs: 1\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxNumTokens: 912\n",
            "[TensorRT-LLM][INFO] TRTGptModel maxInputLen: 113 = min(maxSequenceLen - 1, maxNumTokens) since context FMHA and usePackedInput are enabled\n",
            "[TensorRT-LLM][INFO] TRTGptModel If model type is encoder, maxInputLen would be reset in trtEncoderModel to maxInputLen: min(maxSequenceLen, maxNumTokens).\n",
            "[TensorRT-LLM][INFO] Capacity Scheduler Policy: GUARANTEED_NO_EVICT\n",
            "[TensorRT-LLM][INFO] Context Chunking Scheduler Policy: None\n",
            "[TensorRT-LLM][INFO] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
            "[TensorRT-LLM][INFO] Loaded engine size: 265 MiB\n",
            "[TensorRT-LLM][INFO] Inspecting the engine to identify potential runtime issues...\n",
            "[TensorRT-LLM][INFO] The profiling verbosity of the engine does not allow this analysis to proceed. Re-build the engine with 'detailed' profiling verbosity to get more diagnostics.\n",
            "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 212.26 MiB for execution context memory.\n",
            "[TensorRT-LLM][INFO] gatherContextLogits: 0\n",
            "[TensorRT-LLM][INFO] gatherGenerationLogits: 0\n",
            "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 350 (MiB)\n",
            "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 18.00 MB GPU memory for runtime buffers.\n",
            "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 2.17 MB GPU memory for decoder.\n",
            "[TensorRT-LLM][INFO] Memory usage when calculating max tokens in paged kv cache: total: 22.16 GiB, available: 21.16 GiB\n",
            "[TensorRT-LLM][INFO] Number of blocks in KV cache primary pool: 17332\n",
            "[TensorRT-LLM][INFO] Number of blocks in KV cache secondary pool: 0, onboard blocks to primary memory before reuse: true\n",
            "[TensorRT-LLM][INFO] Number of tokens per block: 32.\n",
            "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 9.52 GiB for max tokens in paged KV cache (277312).\n",
            "[TensorRT-LLM][INFO] Number of tokens per block: 32.\n",
            "[TensorRT-LLM][INFO] [MemUsageChange] Allocated 9.52 GiB for max tokens in paged KV cache (277312).\n",
            "[TensorRT-LLM][INFO] This is an Encoder-Decoder model, set 0.5 cross KV cache fraction based on the config.\n",
            "[TensorRT-LLM][INFO] Number of blocks in self KV cache primary pool: 8666, in cross KV cache primary pool: 8666\n",
            "[TensorRT-LLM][INFO] Number of blocks in self KV cache secondary pool: 0, in cross KV cache secondary pool: 0\n",
            "README.md: 100% 520/520 [00:00<00:00, 2.74MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 9.19M/9.19M [00:00<00:00, 136MB/s]\n",
            "Generating validation split: 100% 73/73 [00:00<00:00, 1369.04 examples/s]\n",
            "[TensorRT-LLM][WARNING] Default padding attention mask will be used as not all requests have cross attention mask.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the request. Default padding attention mask will be created.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the request. Default padding attention mask will be created.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the request. Default padding attention mask will be created.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the request. Default padding attention mask will be created.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "[TensorRT-LLM][WARNING] CrossAttentionMask is not provided for the generation request. Full valid attentionMask will be used by default.\n",
            "prediction:  Yet, these thoughts affected Hester Prynne less with hope than apprehension.\n",
            "RTF: 0.1587\n",
            "total_duration: 4.825 seconds\n",
            "(0.00 hours)\n",
            "processing time: 0.766 seconds (0.00 hours)\n",
            "batch size: 4\n",
            "num_beams: 1\n",
            "total error rate: 1100.00%\n",
            "\n",
            "[TensorRT-LLM][INFO] Refreshed the MPI local session\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "## 8. Triton Inference Server Setup\n",
        "Preparing the optimized model for deployment on NVIDIA Triton Inference Server. This involves creating the necessary directory structure and configuration files.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4yxynF_WTw0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p triton_models/whisper/1\n",
        "!mkdir -p triton_models/whisper_encoder/1\n",
        "!mkdir -p triton_models/whisper_decoder/1"
      ],
      "metadata": {
        "id": "vYrNlI6pLRV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/TensorRT-LLM.zip /content/TensorRT-LLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbriOXTePrr5",
        "outputId": "4e5d7a2b-d1f1-4f47-d34d-437094287730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_128_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_96_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_72_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_80_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_32_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_bf16_256_64_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_128_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_104_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_104_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_80_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_48_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_80_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_qkv_64_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_104_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_104_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_96_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_96_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_paged_kv_160_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_q_paged_kv_40_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_64_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_kv_64_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_192_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_80_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_paged_kv_16_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_48_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_q_paged_kv_48_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_q_paged_kv_32_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_160_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_40_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_32_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_qkv_48_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_qkv_48_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_qkv_256_tma_ws_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_192x128_sm100.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_128_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_192_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_256_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_576x512_sm100.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_qkv_192_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_q_paged_kv_256_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_64_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_256_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_kv_64_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_kv_128_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_128_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_576x512_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_64_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_128_S_q_paged_kv_160_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_q_kv_32_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_qkv_16_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_qkv_256_softcapping_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_40_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_256_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_qkv_40_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_96_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_256_softcapping_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_32_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_128_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_48_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_128_softcapping_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_96_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_paged_kv_48_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_qkv_32_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_128_softcapping_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_kv_72_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_40_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_96_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_32_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_qkv_40_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_paged_kv_192_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_paged_kv_32_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_192_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_192_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_48_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_128_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_q_paged_kv_16_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_128_S_qkv_192_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_128_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_kv_64_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_q_paged_kv_48_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_kv_64_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_128_S_qkv_160_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_paged_kv_32_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_256_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_kv_32_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_paged_kv_80_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_32_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_qkv_16_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_128_softcapping_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_160_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_192_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_paged_kv_64_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_128_S_qkv_256_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_fp32_256_32_ldgsts_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_80_sage_64_64_256_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_128_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_128_S_q_paged_kv_192_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_80_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_paged_kv_192_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_256_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_256_tma_ws_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_104_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_256_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_192_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_kv_64_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_64_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_q_paged_kv_32_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_qkv_256_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_40_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_40_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_96_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_96_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_fp32_128_64_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_128_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_160_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_384_64_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_q_paged_kv_160_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_256_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_qkv_192_tma_ws_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_96_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_64_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_paged_kv_48_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_128_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_128_softcapping_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_192_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_72_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_qkv_160_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_paged_kv_48_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_q_paged_kv_256_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_32_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_kv_64_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_kv_64_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_104_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_256_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_q_kv_32_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_q_paged_kv_256_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_paged_kv_128_softcapping_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_128_qk_tanh_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_qkv_256_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_256_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_kv_128_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_128_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_q_paged_kv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_96_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_96_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_256_softcapping_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_40_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_16_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_32_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_128_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_256_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_64_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_kv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_40_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_128_softcapping_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_72_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_72_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_128_softcapping_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_96_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_q_paged_kv_48_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_q_paged_kv_256_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_192_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_fp32_512_64_ldgsts_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_96_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_160_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_128_softcapping_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_paged_kv_80_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_qkv_32_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_48_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_fp32_256_64_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_104_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_256_softcapping_tma_ws_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_80_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_32_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_128_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_128_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_128_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_kv_32_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_kv_32_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_96_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_fp32_384_32_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_96_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_qkv_48_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_128_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_256_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_128_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_qkv_160_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_q_paged_kv_48_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_32_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_qkv_256_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_104_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_fp32_384_64_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_80_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_paged_kv_256_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_kv_128_qk_tanh_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_128_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_96_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_128_128_S_qkv_40_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_16_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_q_paged_kv_16_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_128_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_192_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_kv_128_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_q_paged_kv_32_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_128_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_q_paged_kv_48_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_160_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_256_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_128_128_S_qkv_64_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_64_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_16_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_192_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_104_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_64_32_S_q_kv_128_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_qkv_48_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_q_kv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_q_paged_kv_40_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_q_paged_kv_40_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_192_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_104_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_kv_64_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_qkv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_64_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_qkv_40_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_128_128_S_q_paged_kv_40_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_64_32_S_q_paged_kv_96_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_192x128_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_104_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_80_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_q_paged_kv_40_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_q_paged_kv_192_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_96_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_qkv_16_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_104_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_96_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_q_paged_kv_192_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_kv_128_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_kv_128_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_qkv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_256_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_80_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_kv_64_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_48_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_qkv_192_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_qkv_40_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_qkv_160_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_80_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_q_paged_kv_16_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_qkv_64_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_96_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_qkv_64_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_kv_128_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_72_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_64_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_128_S_qkv_192_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_qkv_16_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_128_softcapping_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_40_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_96_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_256_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_128_softcapping_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_96_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_bf16_128_64_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_128_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_q_kv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_160_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_qkv_32_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_paged_kv_48_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_kv_64_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_qkv_16_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_64_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_q_paged_kv_32_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_kv_128_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_80_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_96_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_104_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_32_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_40_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_128_128_S_qkv_48_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_128_128_S_q_paged_kv_48_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_kv_128_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_104_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_kv_32_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_64_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_qkv_32_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_q_kv_32_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_paged_kv_40_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_q_paged_kv_256_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_kv_128_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_256_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_256_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_104_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_qkv_32_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_256_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_192_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_128_softcapping_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_80_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_192_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_32_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_kv_128_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_128_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_qkv_16_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_256_softcapping_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_128_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_96_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_96_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_bf16_512_32_ldgsts_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_128_softcapping_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_256_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_fp32_512_32_ldgsts_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_kv_128_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_q_paged_kv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_kv_128_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_paged_kv_104_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_256_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_128_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_80_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_384_32_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_80_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_40_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_576x512_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_160_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_48_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_qkv_48_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_bf16_256_32_ldgsts_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_104_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_128_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_80_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_q_paged_kv_64_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_80_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_40_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_kv_32_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_64_32_S_q_paged_kv_192_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_paged_kv_128_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_48_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_qkv_64_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_qkv_160_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_80_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_256_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_160_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_64_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_48_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_48_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_kv_64_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_q_paged_kv_256_softcapping_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_104_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_qkv_32_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_48_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_256_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_160_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_128_S_q_paged_kv_160_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_128_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_104_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_128_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_128_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_160_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_kv_128_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_bf16_128_32_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_160_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_qkv_192_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_q_paged_kv_160_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_80_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_q_kv_32_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_40_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_128_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_96_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_kv_32_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_16_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_192_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_16_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_96_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_q_kv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_64_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_48_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_160_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_256_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_104_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_64_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_96_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_128_softcapping_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_kv_72_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_256_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_paged_kv_48_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_192_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_80_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_kv_64_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_80_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_paged_kv_32_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_512_32_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_128_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_256_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_qkv_16_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_256_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_q_paged_kv_192_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_80_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_160_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_qkv_64_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_q_paged_kv_16_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_paged_kv_64_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_576x512_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_80_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_qkv_160_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_96_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_q_paged_kv_48_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_fp32_64_32_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_paged_kv_80_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_64_32_S_q_paged_kv_80_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_qkv_16_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_qkv_40_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_96_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_64_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_64_32_S_q_paged_kv_104_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_kv_128_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_q_kv_32_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_48_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_160_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_32_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_128_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_paged_kv_104_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_32_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_128_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_q_kv_128_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_192_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_96_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_256_32_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_104_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_128_softcapping_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_80_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_96_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_64_S_q_paged_kv_160_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_80_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_paged_kv_256_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_q_paged_kv_40_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_80_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_qkv_256_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_192_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_96_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_104_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_160_tma_ws_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_paged_kv_32_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_96_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_qkv_48_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_104_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_q_paged_kv_32_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_256_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_104_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_80_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_192_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_128_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_96_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_160_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_104_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_64_32_S_qkv_256_sm89.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_128_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_qkv_256_softcapping_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_40_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_q_paged_kv_64_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_192x128_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_96_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_64_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_256_S_q_kv_64_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_256_qk_tanh_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_qkv_32_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_80_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_kv_128_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_q_paged_kv_80_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_bf16_384_32_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_fp16_fp32_64_64_ldgsts_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_qkv_192_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_q_paged_kv_80_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_104_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_48_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_80_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_32_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_80_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_160_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_128_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_qkv_256_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_104_sm120.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_96_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_48_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_104_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_40_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_128_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_128_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_qkv_16_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_256_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_q_paged_kv_256_softcapping_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_q_paged_kv_160_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_64_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_qkv_256_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_qkv_64_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_q_paged_kv_192_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_256_softcapping_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_80_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_64_32_S_qkv_192_sm89.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_104_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_72_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_q_kv_64_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_80_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_48_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_64_S_q_paged_kv_256_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_q_paged_kv_160_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_256_softcapping_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_qkv_40_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_128_S_qkv_160_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_72_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_qkv_40_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_80_alibi_tma_ws_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_256_S_q_kv_64_alibi_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_qkv_192_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_192_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_64_S_qkv_160_tma_ws_sm90.cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_kv_32_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_256_S_q_kv_64_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_128_S_qkv_256_softcapping_sm80.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_128_128_S_q_kv_32_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_q_paged_kv_256_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_64_sm89.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_64_256_S_qkv_104_tma_ws_sm90.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_128_128_S_q_kv_64_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_q_paged_kv_256_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_160_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_128_128_S_qkv_48_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_qkv_64_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_128_S_qkv_256_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_32_S_qkv_104_sm80.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_16_S_q_paged_kv_192_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_fp32_64_32_S_qkv_128_softcapping_sm86.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_32_S_qkv_80_sm90.cubin.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_64_32_S_qkv_104_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_e4m3_fp32_64_32_S_qkv_128_sm89.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_64_16_S_q_paged_kv_256_softcapping_sm86.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_bf16_128_128_S_q_paged_kv_64_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/cubin/fmha_v2_flash_attention_fp16_64_16_S_qkv_192_sm120.cubin.cpp (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fmhaPackedMask.h (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/CMakeLists.txt (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fused_multihead_attention_common.h (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/samplingTopKKernels.cu (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/allReduceFusionKernels.cu (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/xqaDispatcher.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/samplingAirTopPKernels.cu (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/doraScaling.cu (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/decodingCommon.cu (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/allReduceFusionKernels.h (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cumsumLastDim.h (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/qserveGemmPerGroup.cu (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/stopCriteriaKernels.cu (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/topkLastDim.cu (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/splitkGroupGemm.cu (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention.cu (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/qserveGemmPerChannel.cu (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/mlaKernels.h (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/groupGemm.h (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/rmsnormKernels.cu (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/mambaConv1dKernels.cu (deflated 87%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.cu (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/lookupKernels.cu (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/preQuantScaleKernel.cu (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/logitsBitmask.cu (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_half.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp8.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_bf16.cu (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_int8.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_template.h (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_float.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp4.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp4.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_int8.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_int8.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_fp8.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp8.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/quantization.cu (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/noAuxTcKernels.cu (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/mambaConv1dKernels.h (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/ipcsocket.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/ipcsocket.h (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/utils.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/userbuffers.cu (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/userbuffers-host.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/ub_interface.cpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/userbuffers.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/ub_interface.h (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/ub_allocator.h (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/CMakeLists.txt (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/userbuffers/ub_allocator.cpp (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.cu (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/noAuxTcKernels.h (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/layernormKernels.cu (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/lruKernel.h (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_preprocessors.h (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/int8_gemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/int8_gemm/int8_gemm_template.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/int8_gemm/int8_gemm_int32.cu (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/int8_gemm/int8_gemm_fp16.cu (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/int8_gemm/int8_gemm.h (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/int8_gemm/int8_gemm_bf16.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/int8_gemm/int8_gemm_fp32.cu (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/allreduce_gemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/allreduce_gemm/allreduce_gemm_impl_sm90.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/python/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/bf16_int4_gemm_fg_scaleonly.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/bf16_int8_gemm_per_col.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/bf16_int8_gemm_fg_scalebias.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/launchers/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/launchers/fpA_intB_launcher_sm90.inl (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/launchers/fpA_intB_launcher_sm90.h (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template_sm90.h (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fp16_int4_gemm_per_col.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/bf16_int4_gemm_per_col.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/e4m3_int4_gemm_fg_scalebias_bf16_out_bf16.cu (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/e4m3_int4_gemm_fg_scaleonly_f16_out_f16.cu (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/e4m3_int4_gemm_fg_scalebias_f16_out_f16.cu (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fp16_int8_gemm_per_col.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/bf16_int8_gemm_fg_scaleonly.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/e4m3_int4_gemm_fg_scaleonly_bf16_out_bf16.cu (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fp16_int4_gemm_fg_scalebias.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/bf16_int4_gemm_fg_scalebias.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fp16_int8_gemm_fg_scaleonly.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fp16_int4_gemm_fg_scaleonly.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/e4m3_int4_gemm_per_col_f16_out_f16.cu (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fp16_int8_gemm_fg_scalebias.cu (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_heuristic.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_kernel_template_sm90.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_bf16.cu (deflated 42%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_kernel_template_sm89.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_fp16.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_template.h (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_heuristic.h (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_preprocessors.cpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_template.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/gemm_swiglu_e4m3.cu (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_kernel_template_sm90.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/banBadWords.h (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/kernelParams.h (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/fmhaRunnerParams.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/fmhaRunner.h (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/fmhaKernels.h (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/fmhaRunner.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/kernelMetaInfo.h (deflated 97%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Static2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk192HV128LayoutPagedKvMaskCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk192HV128LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskCustomVarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk192HV128LayoutPackedQkvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP64VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE2m1AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ64TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk128HV128LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk128HV128LayoutPackedQkvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk32HV32LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk32HV32LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP32VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk32HV32LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ64TileSizeKv64Persistent2CtaKeepsMmaAbForGeneration_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk32HV32LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ128TileSizeKv128PersistentKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk32HV32LayoutPagedKvMaskCausalP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta256LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ64TileSizeKv128StaticKeepsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP32VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutContiguousKvMaskDenseMultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk128HV128LayoutPagedKvMaskDenseP128VarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskCustomP128VarSeqLenTileSizeQ128TileSizeKv128PersistentSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32VarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ8TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPagedKvMaskSlidingWindowCausalP128VarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ8TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk64HV64LayoutContiguousKvMaskCustomMultiCtasKvModeVarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE4m3HQk64HV64LayoutPackedQkvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128StaticContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP32VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk64HV64LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP32MultiCtasKvModeVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk64HV64LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta128LayoutPagedKvMaskDenseP128MultiCtasKvModeVarSeqLenTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk576HV512HVPerCta512LayoutContiguousKvMaskDenseVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QBfloat16KvBfloat16AccFp32OBfloat16HQk576HV512HVPerCta256LayoutContiguousKvMaskDenseVarSeqLenTileSizeQ8TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OBfloat16HQk64HV64LayoutPagedKvMaskCustomP64VarSeqLenTileSizeQ128TileSizeKv128StaticSpecDecodingGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OFp16HQk128HV128LayoutPagedKvMaskDenseP64VarSeqLenTileSizeQ16TileSizeKv128PersistentSwapsMmaAbForGeneration_cubin.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QFp16KvFp16AccFp32OFp16HQk576HV512HVPerCta512LayoutPagedKvMaskDenseP64MultiCtasKvModeVarSeqLenReuseSmemKForVTileSizeQ16TileSizeKv64StaticSwapsMmaAbForGeneration_cubin.cpp (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/cubin/FmhaSm100Kernel_QE4m3KvE4m3AccFp32OE2m1HQk128HV128LayoutContiguousKvMaskSlidingWindowCausalVarSeqLenTileSizeQ128TileSizeKv128PersistentContext_cubin.cpp (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fmha/CMakeLists.txt (deflated 40%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/kernelParams.h (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/kernelRunner.h (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/kernelList.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/kernelRunner.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/cubin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/cubin/GemmKernel_Bfloat16_E4m3_Fp32_tile128x128x128_epilogueTile128x64_mma128x64x32_cluster1x1x1_dsFp8_sm100a_cubin.h (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/cubin/GemmKernel_Fp32_E4m3_Fp32_tile128x128x128_epilogueTile128x64_mma128x64x32_cluster1x1_dsFp8_sm100a_cubin.h (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/cubin/GemmKernel_E4m3_E4m3_Fp32_tile128x128x128_epilogueTile128x64_mma128x64x32_cluster1x1_dsFp8_sm100a_cubin.h (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/blockscaleGemm/CMakeLists.txt (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/gemmCommon.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/runner.cu (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/runner.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/Dtype.h (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/KernelParams.h (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/TmaDescriptor.h (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/macros.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/DtypeDecl.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/DevKernel.h (deflated 88%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/Enums.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/RoutingKernel.cu (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/KernelTraits.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/Utils.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/DevKernel.cu (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/trtllmGenSrc/RoutingKernel.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/gemmList.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/gemmCubins/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/gemmCubins/MoE_ProjDown__BatchN_E4m3Fp32_E4m3_Tile128x128x128_EpiTile64x128_Mma64x128x32_Cluster1x1x1_transposeMmaOutput_DsFp8_sm100a_cubin.h (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/gemmCubins/MoE_ProjUp__BatchN_E4m3Fp32_E4m3_Tile128x128x128_EpiTile64x128_Mma64x128x32_Cluster1x1x1_transposeMmaOutput_DsFp8_InplaceRoute_sm100a_cubin.h (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/gemmCubins/MoE_ProjDown__BatchN_E4m3Fp32_Bfloat16_Tile128x128x128_EpiTile64x128_Mma64x128x32_Cluster1x1x1_transposeMmaOutput_DsFp8_sm100a_cubin.h (deflated 94%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/fp8BlockScaleMoe/CMakeLists.txt (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/trtllmGenKernels/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/lookupKernels.h (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/mlaKernels.cu (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/CMakeLists.txt (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/banRepeatNgram.h (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/kvCachePartialCopy.cu (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/customAllReduceKernels.h (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/kernels/sageAttentionKernels.h (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/gptDecoder.cpp (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/tllmBuffers.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/eagleBuffers.cpp (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/runtimeKernels.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/ipcNvlsMemory.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/utils/numpyUtils.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/utils/sessionUtils.h (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/utils/speculativeChoicesUtils.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/utils/mpiUtils.cpp (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/utils/sessionUtils.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/utils/numpyUtils.h (deflated 46%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/utils/debugUtils.cu (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/ipcSocket.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/transformerBuffers.h (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/tensorView.h (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/tllmBuffers.cpp (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/loraUtils.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/statefulGptDecoder.h (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/generationConfig.cpp (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/cudaMemPool.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/torch.h (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/layerProfiler.cpp (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/runtimeKernels.cu (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/gptJsonConfig.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/decodingLayerWorkspace.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/iBuffer.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/bufferView.h (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/decodingOutput.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/runtimeBuffers.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/torchView.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/layerProfiler.h (deflated 46%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/explicitDraftTokensBuffers.cpp (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/loraUtils.h (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/workerPool.cpp (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/generationConfig.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/memoryCounters.cpp (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/loraManager.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/rnnStateBuffers.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/explicitDraftTokensModule.h (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/transformerBuffers.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/promptTuningParams.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/iTensor.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/ncclCommunicator.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/ipcSocket.h (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/statefulGptDecoder.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/gptDecoderBatched.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/gptSession.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/tllmLogger.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/worldConfig.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/runtimeBuffers.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/statefulGptDecoderBatched.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/loraCache.cpp (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/tllmRuntime.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/loraManager.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/ncclCommunicator.cpp (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/lookaheadBuffers.cpp (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/decodingLayerWorkspace.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/bufferManager.cpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/cudaMemPool.h (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/ipcUtils.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/workerPool.h (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/tllmRuntime.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/torchUtils.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/loraModule.cpp (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/CMakeLists.txt (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/rnnStateBuffers.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/runtime/jsonSerialization.h (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/arch/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/arch/copy_traits_sm90_multimem.hpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/arch/mma.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/arch/copy_sm90_multimem.hpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/arch/copy_red_global.hpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/arch/grid_dependency_control.h (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/weight_only_quant_op.h (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/util/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/util/gather_tensor.hpp (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue_helpers.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/warp/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/warp/mma_tensorop_compute_B_with_f16.h (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/warp/default_mma_tensor_op.h (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/warp/mma_tensorop_dequantizer.h (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/default_dq_mma.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/default_mma_bf16.h (deflated 87%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/dq_mma_multistage_percol.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/dq_mma_base.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/default_mma.h (deflated 87%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/dq_mma_pipelined_percol.h (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/default_dq_mma_pipelined.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/default_dq_mma_multistage.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/dq_mma_multistage.h (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/dq_mma_multistage_finegrained.h (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/dq_mma_pipelined.h (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/threadblock/dq_mma_pipelined_finegrained.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/builders/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/builders/sm90_gmma_builder_gated.inl (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/builders/sm90_gmma_builder_interleaved.inl (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/collective_builder_gated.hpp (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/collective_mma_interleaved.hpp (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/sm90_mma_gated_tma_gmma_ss_warpspecialized_fp8.hpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/collective_builder_interleaved.hpp (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/sm90_mma_gated_tma_gmma_ss_warpspecialized.hpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/sm90_mma_interleaved_tma_gmma_rs_warpspecialized_mixed_input.hpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/collective/collective_mma_gated.hpp (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fpA_intB_gemm.h (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/gemm_with_epilogue_visitor.h (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/sm90_gemm_allreduce_tma_warpspecialized.hpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/sm90_gemm_allreduce_tma_warpspecialized_pingpong.hpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/splitk_gemm_grouped.h (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/mixed_gemm_B_layout.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/gemm_moe_problem_visitor.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/gemm_universal_allreduce.hpp (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/default_int8_traits.h (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/sm90_gemm_gated_tma_warpspecialized_pingpong.hpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel_traits.cuh (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel_routine.cuh (deflated 88%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/default_splitk_gemm_grouped.h (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/gemm_universal_gated.hpp (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/sm90_gemm_allreduce_tma_warpspecialized_cooperative.hpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/default_fpA_intB_traits.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel.cuh (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/moe_problem_visitor.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/moe_cutlass_kernel.h (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/moe_cute_util.cuh (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/sm90_gemm_gated_tma_warpspecialized_cooperative.hpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/device/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/device/splitk_gemm_grouped.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/device/gemm_universal_base_compat.h (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/tile_interleaved_layout.h (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/threadblock/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/threadblock/epilogue_tensor_op_int32.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/threadblock/epilogue_per_row_per_col_scale.h (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/fusion/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/fusion/sm90_visitor_allreduce_tma_warpspecialized.hpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/collective/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/collective/epilogue_moe_finalize.hpp (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/thread/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/epilogue/thread/fused_activations.h (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/transform/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/transform/threadblock/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/transform/threadblock/fine_grained_scale_zero_iterator.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/communication/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/communication/collective/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/communication/collective/sm90_allreduce_nvls_warpspecialized.hpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/compute_occupancy.h (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm_configs.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/system_barrier.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/interleaved_numeric_conversion.h (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/disaggServerUtil.cpp (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/logitsPostProcessorConfig.cpp (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/intervalSet.h (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/samplingConfig.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/model.h (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/peftCacheConfig.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/jsonSerialization.cpp (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/mropeConfig.cpp (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/responseImpl.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/response.cpp (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/requestUtils.h (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/cacheConcatenate.cu (deflated 87%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/cacheConcatenate.h (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/mpi_utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/mpi_utils/connection.cpp (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/mpi_utils/connection.h (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/ucx_utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/ucx_utils/connection.cpp (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/ucx_utils/CMakeLists.txt (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/cache_transmission/ucx_utils/connection.h (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/parallelConfig.cpp (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/executor.cpp (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/executorKVCacheEventManager.cpp (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/serializeUtils.h (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/speculativeDecodingConfig.cpp (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/extendedRuntimePerfKnobConfig.cpp (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/contextPhaseParams.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/request.cpp (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/schedulerConfig.cpp (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/tensor.cpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/dynamicBatchTuner.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/orchestratorUtils.h (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/orchestratorConfig.cpp (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/executorConfig.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/requestWithId.h (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/guidedDecodingConfig.cpp (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/debugConfig.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/requestWithId.cpp (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/requestImpl.h (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/kvCacheConfig.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/executorImpl.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/decodingConfig.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/types.cpp (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/dynamicBatchConfig.cpp (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/dynamicBatchTuner.cpp (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/executorImpl.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/kvCacheRetentionConfig.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/serialization.cpp (deflated 87%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/CMakeLists.txt (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/loraConfig.cpp (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/guidedDecodingParams.cpp (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/promptTuningConfig.cpp (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/outputConfig.cpp (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/executor/requestUtils.cpp (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmAllReducePlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmAllReducePlugin/gemmAllReducePlugin.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmAllReducePlugin/gemmAllReducePlugin.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmAllReducePlugin/gemmAllReducePluginProfiler.h (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmAllReducePlugin/gemmAllReducePluginResource.h (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmAllReducePlugin/gemmAllReducePluginProfiler.cpp (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmAllReducePlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/common/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/common/plugin.cpp (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/common/gemmPluginProfiler.h (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/common/gemmPluginProfiler.cpp (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/common/plugin.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/common/checkMacrosPlugin.cpp (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/common/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/common/checkMacrosPlugin.h (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/common/pluginUtils.h (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cumsumLastDimPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cumsumLastDimPlugin/cumsumLastDimPlugin.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cumsumLastDimPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cumsumLastDimPlugin/cumsumLastDimPlugin.cpp (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/eaglePlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/eaglePlugin/eagleDecodeDraftTokensPlugin.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/eaglePlugin/eagleSampleAndAcceptDraftTokensPlugin.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/eaglePlugin/eagleDecodeDraftTokensPlugin.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/eaglePlugin/eagleSampleAndAcceptDraftTokensPlugin.cpp (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/eaglePlugin/eaglePrepareDrafterInputsPlugin.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/eaglePlugin/eaglePrepareDrafterInputsPlugin.h (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/eaglePlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/allgatherPlugin.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/sendPlugin.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/reduceScatterPlugin.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/allreducePlugin.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/recvPlugin.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/allgatherPlugin.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/sendPlugin.h (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/allreducePlugin.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/reduceScatterPlugin.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/ncclPlugin/recvPlugin.h (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gptAttentionPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gptAttentionPlugin/gptAttentionPlugin.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gptAttentionPlugin/gptAttentionPlugin.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gptAttentionPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmPlugin/gemmPlugin.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmPlugin/gemmPlugin.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lookupPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lookupPlugin/lookupPlugin.cpp (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lookupPlugin/lookupPlugin.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lookupPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/rmsnormQuantizationPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/rmsnormQuantizationPlugin/rmsnormQuantizationPlugin.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/rmsnormQuantizationPlugin/rmsnormQuantizationPlugin.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/rmsnormQuantizationPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/doraPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/doraPlugin/doraPlugin.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/doraPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/doraPlugin/doraPlugin.cpp (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/selectiveScanPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/selectiveScanPlugin/selectiveScanPlugin.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/selectiveScanPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/selectiveScanPlugin/selectiveScanPlugin.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizeTensorPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizeTensorPlugin/quantizeTensorPlugin.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizeTensorPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizeTensorPlugin/quantizeTensorPlugin.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.cpp (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/weightOnlyQuantMatmulPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/weightOnlyQuantMatmulPlugin/weightOnlyQuantMatmulPlugin.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/weightOnlyQuantMatmulPlugin/weightOnlyQuantMatmulPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/weightOnlyQuantMatmulPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizePerTokenPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizePerTokenPlugin/quantizePerTokenPlugin.cpp (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizePerTokenPlugin/quantizePerTokenPlugin.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizePerTokenPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/identityPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/identityPlugin/identityPlugin.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/identityPlugin/identityPlugin.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/identityPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/topkLastDimPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/topkLastDimPlugin/topkLastDimPlugin.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/topkLastDimPlugin/topkLastDimPlugin.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/topkLastDimPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cpSplitPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cpSplitPlugin/cpSplitPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cpSplitPlugin/cpSplitPlugin.cpp (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cpSplitPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lruPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lruPlugin/lruPlugin.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lruPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lruPlugin/lruPlugin.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/mambaConv1dPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/mambaConv1dPlugin/mambaConv1dPlugin.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/mambaConv1dPlugin/mambaConv1dPlugin.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/mambaConv1dPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cudaStreamPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cudaStreamPlugin/cudaStreamPlugin.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cudaStreamPlugin/cudaStreamPlugin.cpp (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/cudaStreamPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/api/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/api/tllmPlugin.cpp (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/qserveGemmPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/qserveGemmPlugin/qserveGemmPlugin.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/qserveGemmPlugin/qserveGemmPlugin.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/qserveGemmPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/loraPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/loraPlugin/loraPlugin.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/loraPlugin/loraPlugin.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/loraPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizeToFP4Plugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizeToFP4Plugin/quantizeToFP4Plugin.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizeToFP4Plugin/quantizeToFP4Plugin.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/quantizeToFP4Plugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fp4GemmPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fp4GemmPlugin/fp4GemmPlugin.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fp4GemmPlugin/fp4GemmPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fp4GemmPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cu (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gptAttentionCommon/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommonImpl.h (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.h (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/gptAttentionCommon/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/layernormQuantizationPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/layernormQuantizationPlugin/layernormQuantizationPlugin.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/layernormQuantizationPlugin/layernormQuantizationPlugin.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/layernormQuantizationPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/exports.map (deflated 46%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/mixtureOfExperts/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/mixtureOfExperts/mixtureOfExpertsPlugin.h (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/mixtureOfExperts/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/mixtureOfExperts/mixtureOfExpertsPlugin.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fusedLayernormPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fusedLayernormPlugin/fusedLayernormPlugin.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fusedLayernormPlugin/fusedLayernormPlugin.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/fusedLayernormPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/smoothQuantGemmPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/smoothQuantGemmPlugin/smoothQuantGemmPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/smoothQuantGemmPlugin/smoothQuantGemmPlugin.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/smoothQuantGemmPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/CMakeLists.txt (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/exports.def (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lowLatencyGemmSwigluPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lowLatencyGemmSwigluPlugin/lowLatencyGemmSwigluPlugin.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lowLatencyGemmSwigluPlugin/lowLatencyGemmSwigluPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/lowLatencyGemmSwigluPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/bertAttentionPlugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/bertAttentionPlugin/bertAttentionPlugin.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/bertAttentionPlugin/bertAttentionPlugin.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/plugins/bertAttentionPlugin/CMakeLists.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/CMakeLists.txt (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/encoderBuffers.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/microBatchScheduler.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/cacheFormatter.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/trtEncoderModel.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/trtGptModelV1.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/allocateKvCache.cpp (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/utils/staticThreadPool.cpp (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/utils/inflightBatchingUtils.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/utils/debugUtils.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/utils/logitsThread.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/utils/debugUtils.h (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/utils/logitsThread.h (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/utils/inflightBatchingUtils.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/utils/staticThreadPool.h (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/trtGptModel.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/sequenceSlotManager.cpp (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/contextProgress.cpp (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/generateRequestOptions.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/rnnStateManager.cpp (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/llmRequest.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/loraBuffers.h (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/cacheTransceiver.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/mlaCacheFormatter.h (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/trtGptModelInflightBatching.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/medusaBuffers.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/kvCacheEventManager.cpp (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/decoderBuffers.cpp (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/guidedDecoder.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/trtGptModelFactory.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/runtimeBuffers.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/createNewDecoderRequests.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/promptTuningBuffers.h (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/dataTransceiverImpl.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/handleGenerationLogits.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/trtEncoderModel.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/promptTuningBuffers.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/assignReqSeqSlots.cpp (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/capacityScheduler.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/rnnStateBuffers.cpp (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/transformerBuffers.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/makeDecodingBatchInputOutput.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/encoderBuffers.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/dataTransceiver.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/cacheFormatter.h (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/trtGptModelInflightBatching.h (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/evictionPolicy.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/loraBuffers.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/dataTransceiver.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/peftCacheManager.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/CMakeLists.txt (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/kvCacheManager.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/handleContextLogits.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/rnnStateBuffers.h (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/trtGptModelV1.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/dataTransceiverImpl.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/kvCacheTransferManager.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/pauseRequests.cpp (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/mlaCacheFormatter.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tensorrt_llm/batch_manager/logitsPostProcessor.cpp (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/deep_gemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/deep_gemm/jit_utils.cuh (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/deep_gemm/mma_utils.cuh (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/deep_gemm/scheduler.cuh (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/deep_gemm/fp8_gemm.cuh (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/deep_gemm/runtime.cuh (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/deep_gemm/utils.cuh (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/deep_gemm/compiler.cuh (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/deep_gemm/tma_utils.cuh (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/tllmException.h (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/quantization.h (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/utils.h (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/cudaFp8Utils.h (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/dataType.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/stringUtils.h (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/logger.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/algorithm.h (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/arrayView.h (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/cudaProfilerUtils.h (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/cudaBf16Wrapper.h (deflated 36%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/assert.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/cudaUtils.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/common/optionalRef.h (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/layers/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/layers/defaultDecodingParams.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/kernels/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/kernels/math.h (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/kernels/kvCachePartialCopy.h (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/kernels/kvCacheIndex.h (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/kernels/decodingCommon.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/modelConfig.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/gptDecoderBatched.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/speculativeDecodingModule.h (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/utils/debugUtils.h (deflated 46%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/utils/multiDeviceUtils.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/utils/mpiUtils.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/utils/speculativeChoicesUtils.h (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/worldConfig.h (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/iBuffer.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/eagleModule.h (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/common.h (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/loraCachePageManagerConfig.h (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/ipcUtils.h (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/gptJsonConfig.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/lookaheadModule.h (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/cudaStream.h (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/eagleBuffers.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/iGptDecoderBatched.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/explicitDraftTokensBuffers.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/bufferManager.h (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/request.h (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/generationInput.h (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/medusaModule.h (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/lookaheadBuffers.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/statefulGptDecoderBatched.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/promptTuningParams.h (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/gptDecoder.h (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/rawEngine.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/cudaEvent.h (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/runtimeDefaults.h (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/loraCache.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/tllmLogger.h (deflated 41%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/samplingConfig.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/memoryCounters.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/iStatefulGptDecoder.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/gptSession.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/decodingOutput.h (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/loraModule.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/speculativeDecodingMode.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/ipcNvlsMemory.h (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/iTensor.h (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/generationOutput.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/runtime/decodingInput.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/executor/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/executor/executor.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/executor/serialization.h (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/executor/disaggServerUtil.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/executor/types.h (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/executor/dataTransceiverState.h (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/executor/tensor.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/executor/cacheCommunicator.h (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/plugins/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/plugins/api/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/plugins/api/tllmPlugin.h (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/cacheTransceiver.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/transformerBuffers.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/llmRequest.h (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/kvCacheTransferManager.h (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/kvCacheUtils.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/assignReqSeqSlots.h (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/handleContextLogits.h (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/common.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/peftCacheManager.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/trtGptModelOptionalParams.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/handleGenerationLogits.h (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/peftCacheManagerConfig.h (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/pauseRequests.h (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/makeDecodingBatchInputOutput.h (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/capacityScheduler.h (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/generateRequestOptions.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/logitsPostProcessor.h (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/microBatchScheduler.h (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/medusaBuffers.h (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/rnnStateManager.h (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/sequenceSlotManager.h (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/contextProgress.h (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/evictionPolicy.h (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/kvCacheManager.h (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/createNewDecoderRequests.h (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/kvCacheEventManager.h (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/runtimeBuffers.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/guidedDecoder.h (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/kvCacheConfig.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/allocateKvCache.h (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/cpp/include/tensorrt_llm/batch_manager/decoderBuffers.h (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/micro_benchmarks/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/micro_benchmarks/gen-moe-benchmark-file.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/micro_benchmarks/mixtureOfExpertsBackendBenchmarkFixture.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/micro_benchmarks/mixtureOfExpertsBackendBenchmarkLauncher.cu (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/micro_benchmarks/CMakeLists.txt (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/micro_benchmarks/README.md (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/cpp/CMakeLists.txt (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/utils/engines.h (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/utils/common.h (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/utils/executorUtils.h (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/utils/common.cpp (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/utils/engines.cpp (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/utils/executorUtils.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/utils/CMakeLists.txt (deflated 42%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/thop/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/thop/CMakeLists.txt (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/thop/thUtilsTest.cpp (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/utils/utilsTest.cpp (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/utils/CMakeLists.txt (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/cudaUtilsTest.cpp (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/memoryUtilsTest.cu (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/cudaProfilerUtilsTest.cpp (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/tllmExceptionTest.cpp (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/stringUtilsTest.cpp (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/timestampUtilsTest.cpp (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/stlUtilsTest.cpp (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/quantizationTest.cpp (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/optionalRefTest.cpp (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/common/CMakeLists.txt (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/randomLlm.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/lookaheadDecodingLayerTest.cpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/dynamicDecodeLayerTest.h (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/topKSamplingLayerTest.cpp (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/dynamicDecodeLayerTest.cpp (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/baseSamplingLayerTest.h (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/layerUtilsTest.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/externalDraftTokensLayerTest.cpp (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/randomLlm.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/explicitDraftTokensLayerTest.cpp (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/lookaheadPoolManagerTest.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/topPSamplingLayerTest.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/lookaheadRandomLlmTest.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/baseSamplingLayerTest.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/samplingLayerTest.cpp (deflated 87%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/explicitDraftTokensLayerTest.h (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/CMakeLists.txt (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/layers/lookaheadAlgorithmTest.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/ropeTest.cu (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/cudaCoreGemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/cudaCoreGemm/cudaCoreGemmKernelTest.cpp (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/smoothQuant/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/smoothQuant/smoothQuantKernelTest.cpp (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/logitsBitmaskTest.cpp (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/decodingKernelTest.cpp (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/weightOnly/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/weightOnly/weightOnlyKernelTest.cpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/stopCriteriaKernelsTest.cpp (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/allReduce/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/allReduce/allReduceFusionTest.cu (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/allReduce/allReduceKernelTest.cu (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/allReduce/gemmAllReduceTest.cu (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/fused_gated_gemm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/fused_gated_gemm/fused_gated_gemm_util.h (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/fused_gated_gemm/gemmSwigluRunnerTest.cu (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/fused_gated_gemm/gemmSwigluKernelTestSm90Fp8.cu (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/CMakeLists.txt (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/mixtureOfExpertsTest.cu (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/shiftKCacheKernelTest.cu (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/banRepeatNGramsKernelsTest.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/sampling/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/sampling/samplingPenaltyTest.cpp (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/sampling/samplingUtilsTest.cu (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/sampling/samplingTest.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/sampling/samplingTopPTest.cpp (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/sampling/samplingTopKTest.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/sampling/samplingTest.h (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/kernels/sampling/samplingAirTopPTest.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/iTensorTest.cpp (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/transposeKVKernelTest.cpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/workerPoolTest.cpp (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/samplingConfigTest.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/userBufferTest.cpp (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/tllmBuffersTest.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/runtimeKernelTest.cpp (deflated 88%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/cudaMemPoolTest.cpp (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/utilsTest.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/samplingTest.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/iBufferTest.cpp (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/worldConfigTest.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/torchTest.cpp (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/loraUtilsTest.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/decodingLayerWorkspaceTest.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/CMakeLists.txt (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/tllmRuntimeTest.cpp (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/runtime/bufferManagerTest.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/kvCacheConfigTest.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/dynamicBatchTunerTest.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/executorTestSmallArbitraryOutputTensors.cpp (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/samplingConfigTest.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/requestTest.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/executorConfigTest.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/tensorTest.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/loraConfigTest.cpp (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/decodingConfigTest.cpp (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/executorTestSmall.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/serializeUtilsTest.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/requestWithIdTest.cpp (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/intervalSetTest.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/CMakeLists.txt (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/executor/responseTest.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/CMakeLists.txt (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/kvCacheManagerTest.cpp (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/capacitySchedulerTest.cpp (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/staticThreadPoolTest.cpp (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/kvCacheUtilsTest.cpp (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/contextProgressTest.cu (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/evictionPolicyTest.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/llmRequestTest.cpp (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/microBatchSchedulerTest.cpp (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/unit_tests/batch_manager/CMakeLists.txt (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/layers/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/layers/medusaDecodeLayerTest.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/layers/eagleLayerTest.cpp (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/layers/eagleLayerTest.h (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/layers/medusaDecodeLayerTest.cpp (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/kernels/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/kernels/eaglePackDataTest.cpp (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/runtime/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/runtime/gptDecoderBatchedTest.cpp (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/runtime/gptDecoderTest.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/runtime/sanitizerTest.cpp (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/runtime/mpiUtilsTest.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/runtime/gptSessionTest.cpp (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/runtime/medusaModuleTest.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/runtime/loraCacheTest.cpp (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/runtime/loraManagerTest.cpp (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/executor/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/executor/disaggExecutorTest.cpp (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/executor/disaggExecutor.h (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/executor/executorTest.cpp (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/executor/executorTest.h (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/executor/encDecTest.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/executor/executorMockTest.cpp (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/executor/CMakeLists.txt (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/CMakeLists.txt (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/guidedDecoderTest.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/cacheTransceiverTest.cpp (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/trtGptModelTest.cpp (deflated 88%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/modelSpec.cpp (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/trtEncoderModelTest.cpp (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/trtGptModelRealDecoderTest.cpp (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/peftCacheManagerTest.cpp (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/modelSpec.h (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/modelSpecBinding.cpp (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/batch_manager/CMakeLists.txt (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/.gitignore (deflated 35%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_enc_dec_engines.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_chatglm_output.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_enc_dec_output.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_recurrentgemma_engines.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_llama_output.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_chatglm_engines.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_mamba_output.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_redrafter_engines.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_gpt_engines.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_mamba_engines.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_redrafter_output.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_medusa_engines.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_hf_gpt_output.py (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_medusa_output.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_eagle_output.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_gpt_output.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_gptj_output.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/io_converter.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_expected_recurrentgemma_output.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_engines_utils.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_gptj_engines.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_llama_engines.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/generate_test_lora_weights.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/scripts/build_eagle_engines.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/models/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/models/mnist.onnx (deflated 12%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/data/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/data/input_tokens_long.npy (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/data/input_tokens.npy (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/data/input_tokens_chatglm2-6b.npy (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/data/input_tokens_chatglm-6b.npy (deflated 88%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/data/input_tokens_glm-10b.npy (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/data/input_tokens_chatglm3-6b.npy (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/data/input_vicuna.npy (deflated 36%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/resources/data/test_model_lora_config.json (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/cpp/tests/README.md (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/.gitignore (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/.dockerignore (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/setup.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/docs/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/Makefile (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/docs/Doxygen (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/docs/make.bat (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/docs/source/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/H200launch.md (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/Falcon180B-H200.md (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/H100vsA100.md (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/H200launch_tps.png (deflated 25%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/TRT_LLM_v0-5-0_H100vA100_1st.png (deflated 15%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/TRT_LLM_v0-5-0_H100vA100_tps.png (deflated 13%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/XQA_ThroughputvsLatency.png (deflated 18%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/Falcon180B-H200_DecvOct.png (deflated 16%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/H200launch_H200vsH100_tps.png (deflated 17%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/tp_ep.png (deflated 15%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/Falcon180B-H200_acc.png (deflated 17%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/moe_structure.png (deflated 4%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/Falcon180B-H200_H200vA100.png (deflated 19%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/Falcon180B-H200_tps.png (deflated 15%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/media/.gitkeep (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/XQA-kernel.md (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/quantization-in-TRT-LLM.md (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/docs/source/blogs/.gitkeep (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/helper.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/docs/source/torch/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/torch/attention.md (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/docs/source/torch/arch_overview.md (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/docs/source/torch/scheduler.md (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/docs/source/torch/adding_new_model.md (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/docs/source/torch/kv_cache_manager.md (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/docs/source/key-features.md (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/medusa_tree.svg (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/TRTLLM_Scheduler_Vis_2.svg (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/Tensor_Parallelism_Vis.svg (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/TRTLLM_Scheduler_Vis_1.svg (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/TRTLLM_Scheduler_Vis_Chunked_Context_1.svg (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/r1-perf.jpeg (deflated 13%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/Pipeline_Parallel_Vis.svg (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/TRTLLM_Scheduler_Vis_3.svg (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/docs/source/media/TRTLLM_Scheduler_Vis_4.svg (deflated 92%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/perf-analysis.md (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/perf-benchmarking.md (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/performance-tuning-guide/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/performance-tuning-guide/introduction.md (deflated 46%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/performance-tuning-guide/benchmarking-default-performance.md (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/performance-tuning-guide/useful-runtime-flags.md (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/performance-tuning-guide/fp8-quantization.md (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/performance-tuning-guide/tuning-max-batch-size-and-max-num-tokens.md (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/performance-tuning-guide/index.rst (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/performance-tuning-guide/deciding-model-sharding-strategy.md (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/performance-tuning-guide/useful-build-time-flags.md (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/docs/source/performance/perf-overview.md (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/docs/source/python-api/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/python-api/tensorrt_llm.quantization.rst (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/docs/source/python-api/tensorrt_llm.plugin.rst (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/docs/source/python-api/tensorrt_llm.layers.rst (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/docs/source/python-api/tensorrt_llm.runtime.rst (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/docs/source/python-api/tensorrt_llm.models.rst (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/docs/source/python-api/tensorrt_llm.functional.rst (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/docs/source/_templates/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/_templates/footer.html (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/docs/source/index.rst (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/docs/source/installation/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/installation/linux.md (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/docs/source/installation/build-from-source-linux.md (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/docs/source/installation/grace-hopper.md (deflated 40%)\n",
            "  adding: content/TensorRT-LLM/docs/source/installation/windows.md (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/docs/source/installation/build-from-source-windows.md (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/docs/source/quick-start-guide.md (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/docs/source/architecture/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/architecture/model-weights-loader.md (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/docs/source/architecture/workflow.md (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/docs/source/architecture/add-model.md (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/docs/source/architecture/overview.md (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/docs/source/architecture/core-concepts.md (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/docs/source/architecture/checkpoint.md (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/docs/source/conf.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/docs/source/overview.md (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/docs/source/release-notes.md (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/docs/source/llm-api/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/llm-api/index.md (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/docs/source/llm-api-examples/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/llm-api-examples/customization.md (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/docs/source/llm-api-examples/index.rst (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/docs/source/llm-api-examples/llm_examples_index.template.rst_ (deflated 30%)\n",
            "  adding: content/TensorRT-LLM/docs/source/commands/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/commands/trtllm-serve.rst (deflated 34%)\n",
            "  adding: content/TensorRT-LLM/docs/source/commands/trtllm-build.rst (deflated 36%)\n",
            "  adding: content/TensorRT-LLM/docs/source/reference/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/reference/memory.md (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/docs/source/reference/troubleshooting.md (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/docs/source/reference/support-matrix.md (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/docs/source/reference/precision.md (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/images/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/images/disaggregated-service_usage.png (deflated 15%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/gpt-attention.md (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/lora.md (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/weight-streaming.md (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/disaggregated-service.md (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/executor.md (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/speculative-decoding.md (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/expert-parallelism.md (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/gpt-runtime.md (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/graph-rewriting.md (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/docs/source/advanced/kv-cache-reuse.md (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/docs/source/torch.md (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/docs/requirements.txt (deflated 21%)\n",
            "  adding: content/TensorRT-LLM/docs/README.md (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/.github/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.github/ISSUE_TEMPLATE/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.github/ISSUE_TEMPLATE/bug_report.yml (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/.github/workflows/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.github/workflows/module-owners.json (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/.github/workflows/auto-assign.yml (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/.github/workflows/auto-close-inactive-issues.yml (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/.github/workflows/blossom-ci.yml (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/.github/workflows/l0-test.yml (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/.pre-commit-config.yaml (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/CONTRIBUTING.md (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/requirements-dev.txt (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/scripts/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/scripts/gen_cuda_headers_for_xqa.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/scripts/bandit.yaml (deflated 24%)\n",
            "  adding: content/TensorRT-LLM/scripts/replace_version.sh (deflated 30%)\n",
            "  adding: content/TensorRT-LLM/scripts/dco_check.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/scripts/build_wheel.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/scripts/build_cpp_examples.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/CODING_GUIDELINES.md (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/.gitmodules (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/requirements-windows.txt (deflated 33%)\n",
            "  adding: content/TensorRT-LLM/.clang-format (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/disaggregated_params.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/module.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_ipc_utils.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/trtllm-llmapi-launch (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/mpi_session.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/build_cache.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/llm_utils.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/llm_args.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/tracer.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/__init__.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/disagg_utils.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/tokenizer.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/utils.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/llm.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/_perf_evaluator.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/llmapi/mgmn_worker_node.py (deflated 42%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/version.py (deflated 36%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/pooling.py (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/mlp.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/cast.py (deflated 46%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/ssm.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/__init__.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/activation.py (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/recurrent.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/linear.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/attention.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/embedding.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/lora.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/language_adapter.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/normalization.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/moe.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/layers/conv.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/prompt_adapter_manager.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/mode.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/utils/__init__.py (deflated 10%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/utils/fp4_utils.py (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/layers.py (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/image_processing.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/quantize_by_modelopt.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/__init__.py (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/functional.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/quantization/quantize.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/inputs/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/inputs/__init__.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/inputs/data.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/inputs/utils.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/inputs/registry.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/network.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/kv_cache_manager.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/generation.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/model_runner.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/session.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/model_runner_cpp.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/enc_dec_model_runner.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/processor_wrapper/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/processor_wrapper/processor_wrapper.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/processor_wrapper/__init__.py (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/processor_wrapper/mllama_processor_wrapper.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/__init__.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/medusa_utils.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/memory_pools/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/memory_pools/pools_kv_cache_manager.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/memory_pools/memory_pools_allocator.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/memory_pools/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/memory_pools/pool.py (deflated 29%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/multimodal_model_runner.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/runtime/redrafter_utils.py (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/utils/__init__.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/utils/data.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/dataclasses/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/dataclasses/reporting.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/dataclasses/statistics.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/dataclasses/engine.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/dataclasses/__init__.py (deflated 10%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/dataclasses/configuration.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/dataclasses/general.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/dataclasses/enums.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/build/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/build/tuning.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/build/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/build/dataclasses.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/build/build.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/build/utils.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/benchmark/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/benchmark/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/benchmark/utils/asynchronous.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/benchmark/utils/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/benchmark/utils/general.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/benchmark/throughput.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/benchmark/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/bench/benchmark/low_latency.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/graph_rewriting.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/ppl.py (deflated 31%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/onnx_utils.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/multimodal_builder.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/plugin_gen.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/shape_infer.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/templates/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/templates/plugin_common.h (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/templates/plugin_common.cpp (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/templates/functional.py.tpl (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/templates/CMakeLists.txt.tpl (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/templates/tritonPlugins.cpp.tpl (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/templates/plugin.h.tpl (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/templates/plugin.cpp.tpl (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/tools/plugin_gen/core.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/profiler.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/top_model_mixin.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/postproc_worker.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/ipc.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/request.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/__init__.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/proxy.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/result.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/worker.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/utils.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/executor/executor.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/solver.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/sharding_spec.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/assertion_node.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/constant_node.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/shape_node.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/normalization_node.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/plugin_nodes/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/plugin_nodes/gpt_attention_node.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/plugin_nodes/normalization_node.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/plugin_nodes/gemm_node.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/plugin_nodes/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/plugin_nodes/look_up_node.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/plugin_nodes/identity_node.py (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/elementwise_node.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/concatenation_node.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/fill_node.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/sharding_strategy.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/select_node.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/shape_consistency.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/shuffle_node.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/activation_node.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/p2p_node.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/node.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/comm_spec.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/unary_node.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/slice_node.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/input_node.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/matmul_node.py (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/cast_node.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/plugin_node.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/output_node.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/reduce_node.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/identity_node.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/gather_node.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/tensor_parallel/softmax_node.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/config.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/simplifier.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/cluster_info.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/parallelization.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/__init__.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/auto_parallel.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/device_mesh.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/shape_info.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/pipeline_graph.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/node_graph.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/utils.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/auto_parallel/runtime_profiling.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/__init__.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/parameter.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/unet_2d_condition.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/weights.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/resnet.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/pp/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/pp/unet_pp.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/pp/groupnorm.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/pp/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/pp/conv2d.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/pp/attention.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/embeddings.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/unet_2d_blocks.py (deflated 88%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/unet/attention.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gptj/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gptj/model.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gptj/config.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gptj/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gptj/convert.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mllama/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mllama/model.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mllama/config.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mllama/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/modeling_utils.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi/model.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi/config.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi/convert.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/model.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/utils/positional_embeddings.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/utils/layers.py (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/utils/sampler.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/utils/transformer.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/utils/__init__.py (deflated 43%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/utils/modules.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/utils/params.py (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/weight.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/config.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/smoothquant.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gemma/convert.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/grok/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/grok/model.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/grok/weight.py (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/grok/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/grok/convert.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/baichuan/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/baichuan/model.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/baichuan/config.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/baichuan/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/baichuan/convert.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/bert/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/bert/model.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/bert/config.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/bert/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/bert/convert.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi3/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi3/model.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi3/config.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi3/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi3/split_weights.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/phi3/convert.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v1/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v1/model.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v1/config.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v1/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v1/convert.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/commandr/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/commandr/model.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/commandr/config.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/commandr/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/dit/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/dit/model.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/dit/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mamba/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mamba/model.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mamba/config.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mamba/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mamba/convert.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/nemotron_nas/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/nemotron_nas/model.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/nemotron_nas/config.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/nemotron_nas/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/nemotron_nas/layer_config.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/nemotron_nas/convert.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/llama/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/llama/model.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/llama/config.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/llama/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/llama/convert.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/redrafter/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/redrafter/drafter.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/redrafter/model.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/redrafter/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/redrafter/redrafter_helper.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/qwen/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/qwen/model.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/qwen/config.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/qwen/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/qwen/utils.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/qwen/convert.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gpt/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gpt/model.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gpt/config.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gpt/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gpt/convert.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mpt/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mpt/model.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mpt/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/convert_utils.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v2/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v2/model.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v2/config.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v2/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/deepseek_v2/convert.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/medusa/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/medusa/model.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/medusa/weight.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/medusa/config.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/medusa/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gptneox/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gptneox/model.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/gptneox/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/dbrx/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/dbrx/model.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/dbrx/config.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/dbrx/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/bloom/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/bloom/model.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/bloom/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/opt/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/opt/model.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/opt/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/__init__.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/cogvlm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/cogvlm/model.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/cogvlm/config.py (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/cogvlm/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/cogvlm/convert.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/eagle/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/eagle/model.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/eagle/config.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/eagle/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/generation_mixin.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/model_weights_loader.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/recurrentgemma/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/recurrentgemma/model.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/recurrentgemma/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/chatglm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/chatglm/model.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/chatglm/config.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/chatglm/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/chatglm/convert.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/stdit/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/stdit/model.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/stdit/config.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/stdit/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/automodel.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/enc_dec/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/enc_dec/model.py (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/enc_dec/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/falcon/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/falcon/model.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/falcon/config.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/falcon/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/falcon/convert.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mmdit_sd3/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mmdit_sd3/model.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mmdit_sd3/config.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/models/mmdit_sd3/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/sampling_params.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/functional.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/serve/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/serve/postprocess_handlers.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/serve/openai_server.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/serve/openai_protocol.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/serve/__init__.py (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/serve/openai_disagg_server.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/python_plugin.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/plugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/plugin/__init__.py (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/plugin/plugin.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/model_config.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/distributed/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/distributed/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/distributed/common.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/distributed/trtllm.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/utils/node_utils.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/utils/quantization_utils.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/utils/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/utils/cuda_graph.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/utils/benchmark.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/utils/logger.py (deflated 25%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/transform.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/_graph.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/sharding.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/fused_mha.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/fusion.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/quantization.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/kvcache.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/visualization.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/__init__.py (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/fused_moe.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/library/collectives.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/transformations/export.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/shim/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/shim/ad_executor.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/shim/demollm.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/shim/__init__.py (deflated 35%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/shim/interface.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/compile/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/compile/compiler.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/compile/__init__.py (deflated 17%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/compile/backends/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/compile/backends/torch_simple.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/compile/backends/__init__.py (deflated 21%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/compile/backends/torch_opt.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/__init__.py (deflated 30%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/models/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/models/factory.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/models/__init__.py (deflated 20%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/models/hf.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/rope.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/dist.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/torch_attention.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/attention_interface.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/triton_kernels/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/triton_kernels/rope.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/triton_kernels/attention_with_kv_cache.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/triton_kernels/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/triton_kernels/attention_with_paged_kv_cache.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/triton_kernels/rms_norm.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/triton_attention.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/__init__.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/torch_libs/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/torch_libs/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/torch_libs/float8_python_api.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/linear.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/fused_moe.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/quant.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/auto_deploy/custom_ops/flashinfer_attention.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/metadata.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/compilation/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/compilation/backend.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/compilation/__init__.py (deflated 15%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/compilation/recover_pass.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/compilation/patterns/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/compilation/patterns/ar_residual_norm.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/compilation/patterns/ub_allreduce.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/compilation/patterns/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/compilation/patterns/residual_add_norm.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/distributed.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pipeline_interface.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/speculative/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/speculative/__init__.py (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/speculative/interface.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/speculative/mtp.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/speculative/utils.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_multimodal_utils.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_utils.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_nemotron.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_qwen.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_mixtral.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_vila.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_mllama.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_mamba_hybrid.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/__init__.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_llama.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_llava_next.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_bert.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_nvsmall.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_multimodal_encoder.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_auto.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/.gitkeep (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/models/modeling_deepseekv3.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/_util.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/cuda_graph_runner.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/resource_manager.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/layerwise_nvtx_marker.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/distributed.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/config.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/kv_cache_transceiver.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/scheduler.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/py_executor.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/llm_request.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/model_engine.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/py_executor_creator.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/pyexecutor/decoder.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/mlp.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/decoder_layer.py (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/gated_mlp.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/mamba.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/linear.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/fused_moe.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/logits_procesor.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/attention.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/rms_norm.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/embedding.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/modules/rotary_embedding.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/custom_op.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/utils.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/llm.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/attention_backend/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/attention_backend/star_flashinfer.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/attention_backend/flashinfer.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/attention_backend/vanilla.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/attention_backend/__init__.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/attention_backend/interface.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/attention_backend/trtllm.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_torch/attention_backend/utils.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_utils.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/commands/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/commands/prune.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/commands/refit.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/commands/serve.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/commands/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/commands/build.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/commands/bench.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/_common.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/logger.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/scaffolding/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/scaffolding/controller.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/scaffolding/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/scaffolding/math_utils.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/scaffolding/scaffolding_llm.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/scaffolding/task.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/scaffolding/worker.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/mapping.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/lora_manager.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tensorrt_llm/builder.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/examples/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/gptj/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/gptj/convert_checkpoint.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/examples/gptj/.gitignore (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/gptj/requirements.txt (deflated 6%)\n",
            "  adding: content/TensorRT-LLM/examples/gptj/README.md (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/examples/mmlu_llmapi.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/mllama/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/mllama/convert_checkpoint.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/examples/mllama/requirements.txt (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/mllama/README.md (deflated 28%)\n",
            "  adding: content/TensorRT-LLM/examples/phi/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/phi/convert_checkpoint.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/examples/phi/requirements.txt (deflated 11%)\n",
            "  adding: content/TensorRT-LLM/examples/phi/README.md (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v3/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v3/README.md (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/internlm2/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/internlm2/convert_checkpoint.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/examples/internlm2/.gitignore (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/internlm2/requirements.txt (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/internlm2/README.md (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/examples/gemma/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/gemma/convert_checkpoint.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/examples/gemma/requirements.txt (deflated 41%)\n",
            "  adding: content/TensorRT-LLM/examples/gemma/README.md (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/examples/smaug/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/smaug/requirements.txt (deflated 11%)\n",
            "  adding: content/TensorRT-LLM/examples/smaug/README.md (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/examples/grok/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/grok/convert_checkpoint.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/grok/requirements.txt (deflated 30%)\n",
            "  adding: content/TensorRT-LLM/examples/grok/README.md (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/examples/baichuan/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/baichuan/convert_checkpoint.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/examples/baichuan/requirements.txt (deflated 21%)\n",
            "  adding: content/TensorRT-LLM/examples/baichuan/README.md (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/.vscode/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/.vscode/launch.json (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/.vscode/extensions.json (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/.vscode/settings.json (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/.vscode/.env (deflated 16%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/simple_config.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/.gitignore (deflated 9%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/CONTRIBUTING.md (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/lm_eval_ad.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/requirements.txt (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/build_and_run_ad.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/build_and_run_flux.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/auto_deploy/README.md (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/convert_checkpoint.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/.gitignore (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/base_benchmark/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/base_benchmark/config.json (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/large_benchmark/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/large_benchmark/config.json (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/large_with_attention_plugin_benchmark/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/large_with_attention_plugin_benchmark/config.json (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/run.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/base_with_attention_plugin_benchmark/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/base_with_attention_plugin_benchmark/config.json (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/utils.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/examples/bert/README.md (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/eval.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/requirements-llava_onevision.txt (deflated 6%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/requirements-qwen2vl.txt (deflated 11%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/requirements-vila.txt (deflated 3%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/requirements-internlm-xcomposer2.txt (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/run.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/build_multimodal_engine.py (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/utils.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/examples/multimodal/README.md (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/examples/sample_weight_stripping/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/sample_weight_stripping/README.md (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/examples/granite/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/granite/README.md (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/examples/mmdit/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/mmdit/assets/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/mmdit/assets/sd3.5-mmdit.png (deflated 0%)\n",
            "  adding: content/TensorRT-LLM/examples/mmdit/convert_checkpoint.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/examples/mmdit/sample.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/examples/mmdit/requirements.txt (deflated 3%)\n",
            "  adding: content/TensorRT-LLM/examples/mmdit/README.md (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v1/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v1/convert_checkpoint.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v1/__init__.py (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v1/requirements.txt (deflated 6%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v1/README.md (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/examples/commandr/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/commandr/convert_checkpoint.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/commandr/requirements.txt (deflated 8%)\n",
            "  adding: content/TensorRT-LLM/examples/commandr/README.md (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/diffusion.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/convert_checkpoint.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/vae_decoder_trt.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/figs/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/figs/sample.png (deflated 0%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/figs/sample.fp8.png (deflated 0%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/utils_modelopt.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/sample.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/examples/dit/README.md (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/mamba/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/mamba/convert_checkpoint.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/examples/mamba/requirements.txt (deflated 12%)\n",
            "  adding: content/TensorRT-LLM/examples/mamba/README.md (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/executorExampleAdvanced.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/inputTokens.csv (deflated 33%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/executorExampleKvEvents.cpp (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/executorExampleDisaggregated.cpp (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/executorExampleFastLogits.cpp (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/executorExampleAdvancedMultiInstances.cpp (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/executorExampleBasic.cpp (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/CMakeLists.txt (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/executorExampleDebug.cpp (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/executorExampleLogitsProcessor.cpp (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp/executor/README.md (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/examples/quantization/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/quantization/requirements.txt (deflated 21%)\n",
            "  adding: content/TensorRT-LLM/examples/quantization/quantize.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/quantization/README.md (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/examples/nemotron_nas/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/nemotron_nas/convert_checkpoint.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/nemotron_nas/calibration_utils.py (deflated 46%)\n",
            "  adding: content/TensorRT-LLM/examples/nemotron_nas/README.md (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/examples/llama/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/llama/convert_checkpoint.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/llama/.gitignore (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/llama/summarize_long.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/llama/requirements.txt (deflated 15%)\n",
            "  adding: content/TensorRT-LLM/examples/llama/README.md (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/examples/mmlu.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/lookahead/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/lookahead/requirements.txt (deflated 11%)\n",
            "  adding: content/TensorRT-LLM/examples/lookahead/README.md (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/examples/redrafter/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/redrafter/convert_checkpoint.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/examples/redrafter/requirements.txt (deflated 11%)\n",
            "  adding: content/TensorRT-LLM/examples/redrafter/README.md (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen/convert_checkpoint.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen/requirements.txt (deflated 29%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen/README.md (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-eval/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-eval/lm-eval-harness/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-eval/lm-eval-harness/lm_eval_tensorrt_llm.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-eval/lm-eval-harness/requirements.txt (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-eval/lm-eval-harness/README.md (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/gpt/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/gpt/convert_checkpoint.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/gpt/.gitignore (deflated 3%)\n",
            "  adding: content/TensorRT-LLM/examples/gpt/run_hf.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/gpt/merge_ptuning_tables.py (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/examples/gpt/nemo_prompt_convert.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/gpt/requirements.txt (deflated 7%)\n",
            "  adding: content/TensorRT-LLM/examples/gpt/nemo_lora_convert.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/gpt/README.md (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/internlm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/internlm/.gitignore (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/internlm/requirements.txt (deflated 12%)\n",
            "  adding: content/TensorRT-LLM/examples/internlm/README.md (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/examples/mpt/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/mpt/convert_checkpoint.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/examples/mpt/.gitignore (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/mpt/requirements.txt (deflated 6%)\n",
            "  adding: content/TensorRT-LLM/examples/mpt/README.md (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v2/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v2/convert_checkpoint.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v2/requirements.txt (deflated 2%)\n",
            "  adding: content/TensorRT-LLM/examples/deepseek_v2/README.md (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/medusa/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/medusa/convert_checkpoint.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/examples/medusa/requirements.txt (deflated 11%)\n",
            "  adding: content/TensorRT-LLM/examples/medusa/README.md (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/assets/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/assets/mel_filters.npz (deflated 3%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/assets/multilingual.tiktoken (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/assets/1221-135766-0002.wav (deflated 18%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/assets/my_whisper_model.pt (deflated 8%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/convert_checkpoint.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_weights_int8/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_weights_int8/decoder/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_weights_int8/decoder/rank0.safetensors (deflated 10%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_weights_int8/decoder/config.json (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_weights_int8/encoder/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_weights_int8/encoder/rank0.safetensors (deflated 14%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_weights_int8/encoder/config.json (deflated 46%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/triton_models/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/triton_models/whisper_encoder/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/triton_models/whisper_encoder/1/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/triton_models/whisper/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/triton_models/whisper/1/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/triton_models/whisper_decoder/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/triton_models/whisper_decoder/1/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/distil_whisper/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/distil_whisper/convert_from_distil_whisper.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_utils.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/run.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/tokenizer.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_int8/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_int8/decoder/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_int8/decoder/config.json (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_int8/decoder/rank0.engine (deflated 12%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_int8/encoder/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_int8/encoder/config.json (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/whisper_model_int8/encoder/rank0.engine (deflated 16%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/requirements.txt (deflated 21%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/__pycache__/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/__pycache__/whisper_utils.cpython-310.pyc (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/__pycache__/tokenizer.cpython-310.pyc (deflated 36%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/model.cache (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/tmp/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/tmp/rtf-librispeech_dummy.txt (deflated 18%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/tmp/recogs-single_wav_test.txt (deflated 22%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/tmp/errs-librispeech_dummy.txt (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/tmp/rtf-single_wav_test.txt (deflated 20%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/tmp/recogs-librispeech_dummy.txt (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/tmp/errs-single_wav_test.txt (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/examples/whisper/README.md (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/examples/gptneox/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/gptneox/convert_checkpoint.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/examples/gptneox/.gitignore (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/gptneox/gptq_convert.sh (deflated 32%)\n",
            "  adding: content/TensorRT-LLM/examples/gptneox/requirements.txt (deflated 6%)\n",
            "  adding: content/TensorRT-LLM/examples/gptneox/README.md (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/examples/constraints.txt (deflated 4%)\n",
            "  adding: content/TensorRT-LLM/examples/dbrx/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/dbrx/convert_checkpoint.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/examples/dbrx/requirements.txt (deflated 9%)\n",
            "  adding: content/TensorRT-LLM/examples/dbrx/README.md (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/generate_checkpoint_config.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/bloom/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bloom/convert_checkpoint.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/examples/bloom/.gitignore (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bloom/requirements.txt (deflated 9%)\n",
            "  adding: content/TensorRT-LLM/examples/bloom/README.md (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/examples/prompt_lookup/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/prompt_lookup/run_dtm_pld.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/prompt_lookup/requirements.txt (deflated 14%)\n",
            "  adding: content/TensorRT-LLM/examples/prompt_lookup/README.md (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/examples/opt/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/opt/convert_checkpoint.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/opt/requirements.txt (deflated 6%)\n",
            "  adding: content/TensorRT-LLM/examples/opt/README.md (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/examples/dora/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/dora/normalize_weights.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/dora/README.md (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/examples/arctic/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/arctic/README.md (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/generate_xgrammar_tokenizer_info.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/examples/cogvlm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/cogvlm/convert_checkpoint.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp_library/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp_library/build.sh (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp_library/main.cpp (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp_library/CMakeLists.txt (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/examples/cpp_library/tensorrt_llm_libutils.h (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/examples/bindings/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bindings/executor/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/bindings/executor/example_logits_processor.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/examples/bindings/executor/example_basic.py (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/examples/bindings/executor/example_debug.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/examples/bindings/executor/example_advanced.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/bindings/executor/README.md (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/examples/jais/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/jais/requirements.txt (deflated 7%)\n",
            "  adding: content/TensorRT-LLM/examples/jais/README.md (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/eagle/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/eagle/convert_checkpoint.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/examples/eagle/requirements.txt (deflated 7%)\n",
            "  adding: content/TensorRT-LLM/examples/eagle/README.md (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/examples/summarize.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/fmha_triton.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/TritonFlashAttentionPlugin.cpp (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/TritonFlashAttentionPlugin.h (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/run.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/plugin.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/build.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/CMakeLists.txt (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/tritonPlugins.cpp (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/manual_plugin/README.md (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/plugin_autogen/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/plugin_autogen/kernel_config.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/plugin_autogen/build_engine.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/plugin_autogen/run_engine.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/plugin_autogen/README.md (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/examples/openai_triton/README.md (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/examples/run.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/examples/exaone/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/exaone/README.md (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/pytorch/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/pytorch/out_of_tree_example/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/pytorch/out_of_tree_example/main.py (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/examples/pytorch/out_of_tree_example/modeling_opt.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/examples/pytorch/quickstart_advanced.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/pytorch/star_attention.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/pytorch/quickstart.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/pytorch/quickstart_multimodal.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/examples/pytorch/README.md (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/examples/blip2/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/blip2/README.md (deflated 35%)\n",
            "  adding: content/TensorRT-LLM/examples/model_api/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/model_api/llama_multi_gpu.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/examples/model_api/llama_quantize.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/model_api/llama.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/examples/model_api/README.md (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/examples/recurrentgemma/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/recurrentgemma/convert_checkpoint.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/examples/recurrentgemma/.gitignore (deflated 20%)\n",
            "  adding: content/TensorRT-LLM/examples/recurrentgemma/requirements.txt (deflated 22%)\n",
            "  adding: content/TensorRT-LLM/examples/recurrentgemma/README.md (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/examples/skywork/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/skywork/requirements.txt (deflated 9%)\n",
            "  adding: content/TensorRT-LLM/examples/skywork/README.md (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/mixtral/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/mixtral/requirements.txt (deflated 8%)\n",
            "  adding: content/TensorRT-LLM/examples/mixtral/README.md (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/chatglm2-6b/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/chatglm2-6b/tokenization_chatglm.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/convert_checkpoint.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/chatglm-6b/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/chatglm-6b/tokenization_chatglm.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/.gitignore (deflated 17%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/glm-4-9b/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/glm-4-9b/tokenization_chatglm.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/chatglm3-6b-32k/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/chatglm3-6b-32k/tokenization_chatglm.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/requirements.txt (deflated 14%)\n",
            "  adding: content/TensorRT-LLM/examples/chatglm/README.md (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/examples/nemotron/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/nemotron/requirements.txt (deflated 16%)\n",
            "  adding: content/TensorRT-LLM/examples/nemotron/README.md (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/hf_lora_convert.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/vit_onnx_trt.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/run_chat.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/run.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/show_pic.py (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/pics/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/pics/1.png (deflated 0%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/pics/demo.jpeg (deflated 3%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/requirements.txt (deflated 22%)\n",
            "  adding: content/TensorRT-LLM/examples/qwenvl/README.md (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/examples/apps/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/apps/chat.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/examples/apps/fastapi_server.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/examples/apps/requirements.txt (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/apps/openai_client.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/examples/apps/README.md (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/examples/utils.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_eagle_decoding.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_inference_customize.py (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_inference_async_streaming.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_multilora.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_quantization.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_inference.py (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_guided_decoding.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_inference_distributed.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_logits_processor.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_auto_parallel.py (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_medusa_decoding.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_lookahead_decoding.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/quickstart_example.py (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_inference_kv_events.py (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/llm_inference_async.py (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/examples/llm-api/README.md (deflated 36%)\n",
            "  adding: content/TensorRT-LLM/examples/draft_target_model/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/draft_target_model/requirements.txt (deflated 11%)\n",
            "  adding: content/TensorRT-LLM/examples/draft_target_model/README.md (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/examples/sdxl/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/sdxl/run_sdxl.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/examples/sdxl/build_sdxl_unet.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/examples/sdxl/pipeline_stable_diffusion_xl.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/examples/sdxl/README.md (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen2audio/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen2audio/audio/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen2audio/audio/glass-breaking-151256.mp3 (deflated 4%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen2audio/run_chat.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen2audio/run.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen2audio/requirements.txt (deflated 22%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen2audio/utils.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/qwen2audio/README.md (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/assets/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/assets/a_beautiful_waterfall.mp4 (deflated 0%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/vae.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/convert_checkpoint.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/text_encoder.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/scheduler.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/sample.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/video_transforms.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/aspect.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/requirements.txt (deflated 27%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/utils.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/pipeline_tllm.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/examples/stdit/README.md (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/examples/python_plugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/python_plugin/run_lookup.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/python_plugin/plugin_lib/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/python_plugin/plugin_lib/lookup_plugin.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/python_plugin/plugin_lib/__init__.py (deflated 19%)\n",
            "  adding: content/TensorRT-LLM/examples/python_plugin/plugin_lib/lookup_kernel.py (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/examples/python_plugin/build_lookup.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/examples/python_plugin/README.md (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/examples/enc_dec/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/enc_dec/helper.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/examples/enc_dec/convert_checkpoint.py (deflated 87%)\n",
            "  adding: content/TensorRT-LLM/examples/enc_dec/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/enc_dec/run.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/examples/enc_dec/README.md (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/examples/infinitebench/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/infinitebench/compute_scores.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/examples/infinitebench/args.py (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/examples/infinitebench/construct_synthetic_dataset.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/examples/infinitebench/eval_utils.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/scaffolding/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/scaffolding/aime24_test.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/examples/scaffolding/test.jsonl (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/scaffolding/scaffolding_llm_run.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/examples/falcon/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/falcon/convert_checkpoint.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/falcon/requirements.txt (deflated 14%)\n",
            "  adding: content/TensorRT-LLM/examples/falcon/README.md (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/disagg_config.yaml (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/launch_disaggregated_server.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/launch_disaggregated_workers.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/clients/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/clients/prompts.json (deflated 32%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/clients/template_trtllm_openai_completions.json (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/clients/disagg_client.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/clients/run_loadgen.sh (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/examples/disaggregated/README.md (deflated 46%)\n",
            "  adding: content/TensorRT-LLM/examples/language_adapter/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/examples/language_adapter/README.md (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/examples/eval_long_context.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/requirements.txt (deflated 37%)\n",
            "  adding: content/TensorRT-LLM/pyproject.toml (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/docker/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docker/Makefile (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/docker/common/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docker/common/install_cmake.sh (deflated 42%)\n",
            "  adding: content/TensorRT-LLM/docker/common/install_polygraphy.sh (stored 0%)\n",
            "  adding: content/TensorRT-LLM/docker/common/install_cuda_toolkit.sh (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/docker/common/install_ccache.sh (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/docker/common/install_pytorch.sh (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/docker/common/install_base.sh (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/docker/common/install_tensorrt.sh (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/docker/common/install_mpi4py.sh (deflated 35%)\n",
            "  adding: content/TensorRT-LLM/docker/Dockerfile.user (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/docker/Dockerfile.multi (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/CODE_OF_CONDUCT.md (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/.git/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/description (deflated 14%)\n",
            "  adding: content/TensorRT-LLM/.git/logs/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/logs/HEAD (deflated 25%)\n",
            "  adding: content/TensorRT-LLM/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/logs/refs/remotes/origin/HEAD (deflated 25%)\n",
            "  adding: content/TensorRT-LLM/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/logs/refs/heads/main (deflated 25%)\n",
            "  adding: content/TensorRT-LLM/.git/index (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/.git/config (deflated 35%)\n",
            "  adding: content/TensorRT-LLM/.git/objects/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/objects/pack/pack-9723c5ee9254db08777ee9dbe930a996527c88ed.pack (deflated 0%)\n",
            "  adding: content/TensorRT-LLM/.git/objects/pack/pack-9723c5ee9254db08777ee9dbe930a996527c88ed.idx (deflated 1%)\n",
            "  adding: content/TensorRT-LLM/.git/objects/info/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/info/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/info/exclude (deflated 28%)\n",
            "  adding: content/TensorRT-LLM/.git/HEAD (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/packed-refs (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/pre-push (deflated 24%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/post-checkout (deflated 26%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/post-merge (deflated 25%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/post-commit (deflated 25%)\n",
            "  adding: content/TensorRT-LLM/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/.git/branches/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/incomplete/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/ce/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/ce/26/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/ce/26/ce26bd65887530d4ab5fe4445cc4589a8a1a71d86601c09569faa30dc72e3442 (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/49/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/49/5b/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/49/5b/495b1be340d4f05cfc82b11c8eda719d6a98f731a78077f5549142125940d50f (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/91/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/91/7c/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/91/7c/917ce44b8845b03e43543c1602330b14b00cc3895df6b42bba39ba72ff8c691d (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/b0/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/b0/1a/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/b0/1a/b01ad8a6f960f21c1b12db19ad3542ad6b0795ec1733a6a5d7da5f99d873c734 (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/02/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/02/cf/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/02/cf/02cf19fb410bf666dc84b930e274158406cd23cd4df9eda6ddadf61b943362e2 (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/1d/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/1d/bf/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/objects/1d/bf/1dbfec57faf92d58e0b35a75dd6157bbb77e04c5f9ddfcbadaac0d922725bc97 (deflated 44%)\n",
            "  adding: content/TensorRT-LLM/.git/lfs/tmp/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/refs/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/.git/refs/heads/main (stored 0%)\n",
            "  adding: content/TensorRT-LLM/README.md (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_module.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_layer.py (deflated 88%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/conftest.py (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_convert_spec_decoding_mask_to_packed_mask.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/utils/runtime_defaults.py (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/utils/llm_data.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/utils/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/utils/cpp_paths.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/utils/test_medusa_utils.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/utils/util.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_plugins.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_llama_conversion.sh (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_precision_control.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/fake.sh (deflated 39%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_executor.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_mpi_session.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/run_llm.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_llm.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_llm_multi_gpu.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_llm_kv_cache_events.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/run_llm_exit.py (deflated 47%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_llm_models.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_build_cache.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/run_llm_with_postproc.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_llm_download.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_llm_perf_evaluator.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_llm_utils.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/mpi_test_task.py (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/grid_searcher.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_llm_server.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_openai_multi_nodes.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/openai_server.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_openai_multi_chat.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_openai_completions.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_openai_misc.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_openai_metrics.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_llm_chat.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_openai_chat.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_openai_multi_gpu.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/_test_openai_consistent_chat.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/apps/README.md (deflated 19%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_llm_quant.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/llmapi/test_llm_models_multi_gpu.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_graph_rewriter.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_debugging_api.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_session.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_moe_weight_only_groupwise_quant_matmul.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_mode.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_smooth_quant_gemm.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_functional.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_weight_only_groupwise_quant_matmul.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_quant_layer.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_fp8_rowwise_gemm.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_fp8_quantization.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_smooth_quant_rms_norm.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_qserve_gemm.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_quant.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/_utils.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_smooth_quant_layer_norm.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/quantization/test_weight_only_quant_matmul.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_pip_install.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/profile_utils.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_leak.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/pytest.ini (deflated 18%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_model_dtype.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/request_output.yaml (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/batched_logits_processor.yaml (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/quant_config.yaml (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/calib_config.yaml (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/logits_processor.yaml (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/sampling_params.yaml (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/guided_decoding_params.yaml (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/completion_output.yaml (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/llm.yaml (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/references/llm_args.yaml (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/api_stability_core.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/api_stability/test_llm_api.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_bloom.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_gptj.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_deepseek.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_llama.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_beams2tree.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_validate.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_prepare_input.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_draft_token_indices.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_top1.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_unpack_gen_data.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_gather_beams.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_process_logits.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_draft_token.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_packed_position_ids.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_prefix_match_indices.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/redrafter/test_mask.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_gpt.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_falcon.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_phi.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_gpt_e2e.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_gptneox.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/eagle/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/eagle/test_sample_accept_draft_tokens_plugin.py (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/eagle/test_decode_draft_tokens_plugin.py (deflated 93%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/eagle/test_prepare_drafter_inputs_plugin.py (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_mamba.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_mistral.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model/test_nemotron_nas.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_pretrained_config.py (deflated 49%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_model_runner_cpp.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/tools/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/tools/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/tools/plugin_gen/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/tools/plugin_gen/test_core.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/tools/plugin_gen/test_plugin_gen.py (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/tools/plugin_gen/kernel_config.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/tools/plugin_gen/__init__.py (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/tools/plugin_gen/test_shape_infer.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_kv_cache_update.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_builder.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_mapping.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/attention/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/attention/test_sage_attention.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/attention/test_deepseek_v2_attention.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/attention/test_gpt_attention.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/attention/test_gpt_attention_no_cache.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/attention/test_bert_attention.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/attention/test_gpt_attention_IFB.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/bindings/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/bindings/test_bindings_ut.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/bindings/test_executor_bindings.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/bindings/binding_test_utils.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_kv_cache_manager.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model_api/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model_api/test_model_level_api.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model_api/profile_utils.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model_api/test_model_api_multi_gpu.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/model_api/test_model_quantization.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/test_export.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/dump_checkpoint_stats.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/thop/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/thop/test_moe_op.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/thop/test_scaled_mm.py (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/thop/test_selective_scan_op.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/thop/test_cublas_mm.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/thop/test_noaux_tc.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/thop/test_mamba_conv1d_op.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/helpers.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_overlap_scheduler_input.json (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu/test_star_attention_input.jsonl (deflated 95%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu/test_deepseek_allreduce.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu/test_star_attention.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu/test_ar_residual_norm.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu/test_user_buffers.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu/test_linear.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu/test_embedding.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/_utils_test/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/_utils_test/_custom_op_utils.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/_utils_test/_dist_test_utils.py (deflated 48%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/_utils_test/_model_test_utils.py (deflated 72%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/_utils_test/_torch_test_utils.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/_utils_test/_graph_test_helpers.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/multigpu/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/multigpu/test_ad_build_small_multi.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/multigpu/transformations/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/multigpu/transformations/library/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/multigpu/transformations/library/test_graph_sharding.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/multigpu/transformations/library/test_collective_fusion.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/multigpu/transformations/library/test_allreduce_residual_rmsnorm_fusion.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/multigpu/custom_ops/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/multigpu/custom_ops/test_dist.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/utils/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/utils/test_quantization_utils.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/utils/test_benchmark_mlp.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/test_ad_build_small_single.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/transformations/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/transformations/test_bf16_gemm.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/transformations/test_export.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/transformations/library/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/transformations/library/test_kv_cache.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/transformations/library/test_fused_mha.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/transformations/library/test_moe_fusion.py (deflated 75%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/transformations/library/test_gemm_fusion.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/transformations/library/test_quantization.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/shim/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/shim/test_engine.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/compile/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/compile/test_torch_opt.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/compile/test_compiler.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/test_ad_moe_op.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/triton_kernels/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/triton_kernels/test_rope.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/triton_kernels/test_attention_with_paged_kv_cache.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/triton_kernels/test_rms_norm.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/triton_kernels/test_attention_with_kv_cache.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/test_flashinfer_attention_op.py (deflated 88%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/test_attention_op.py (deflated 84%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/unit/singlegpu/custom_ops/test_quant.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/integration/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/integration/test_ad_build.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/auto_deploy/integration/test_lm_eval.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_fused_moe.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_pytorch_model_engine.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/pattern_watcher.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/compilation/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/compilation/test_add_norm.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_flashinfer_star_attn.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_fp8_linear.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_fp4_linear.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_fp8_quantize.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_vanilla_attention.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu_modeling/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu_modeling/test_deepseek.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/multi_gpu_modeling/test_llama.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/speculative/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/speculative/test_mtp_prepare_drafter_inputs.py (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/speculative/test_mtp_sample_and_accept_draft_tokens.py (deflated 89%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/speculative/test_mtp_update_mtp_hidden_states.py (deflated 91%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_bert.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_mllama.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_llama.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_out_of_tree.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_nemotron.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_qwen.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_mixtral.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_vila.py (deflated 85%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_mamba_hybrid.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/modeling/test_modeling_nvsmall.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_attention.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_fp4_gemm_quantize.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_overlap_scheduler.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_moe_routing.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_fp8_block_scale_gemm.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_fp4_bmm_quantize.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_moe.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/_torch/test_flashinfer_attention.py (deflated 83%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/python_plugin/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/python_plugin/test_plugin_wrapper.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/python_plugin/plugin_wrapper_utils.py (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/ (stored 0%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_pp_reduce_scatter.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_nonzero.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_expand.py (deflated 77%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_allreduce_prepost_residual_norm.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_flatten.py (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_softplus.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_identity.py (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_flip.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_view.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_logsoftmax.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_conv2d.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_interpolate.py (deflated 90%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_transpose.py (deflated 52%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_allreduce_norm.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_gather_nd.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_rearrange.py (deflated 60%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_rg_lru.py (deflated 78%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_low_latency_gemm.py (deflated 63%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_masked_scatter.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_geglu.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_conv3d.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_topk.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_nccl.py (deflated 64%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_select.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_scatter_nd.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_dora.py (deflated 79%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_einsum.py (deflated 58%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_sigmoid.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_group_norm.py (deflated 55%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_sample.py (deflated 67%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_lora.py (deflated 69%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_repeat_interleave.py (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_repeat.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_fp4_gemm.py (deflated 81%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_cast.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_mamba_conv1d.py (deflated 76%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_gemm_swiglu.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_meshgrid2d.py (deflated 59%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_assertion.py (deflated 57%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_sin.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_index_select.py (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_silu.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_arange.py (deflated 66%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_fp4_gemm_ootb.py (deflated 74%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_swiglu.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_gelu.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_outer.py (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_permute.py (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_exp.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/torch_ref.py (deflated 80%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_gather.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_cumsum.py (deflated 73%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_masked_select.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_argmax.py (deflated 53%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_scatter.py (deflated 65%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_matmul.py (deflated 70%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_unbind.py (deflated 71%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_alibi.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_quant.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_unsqueeze.py (deflated 54%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_embedding_single_gpu.py (deflated 61%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_selective_scan.py (deflated 86%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_where.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_cos.py (deflated 51%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_squeeze.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_moe.py (deflated 82%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_split.py (deflated 62%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_slice.py (deflated 68%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_avg_pool2d.py (deflated 56%)\n",
            "  adding: content/TensorRT-LLM/tests/unittest/functional/test_pad.py (deflated 57%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy your encoder engine files\n",
        "!cp /content/TensorRT-LLM/examples/whisper/whisper_model_int8/encoder/* triton_models/whisper_encoder/1/\n",
        "\n",
        "# Copy your decoder engine files\n",
        "!cp /content/TensorRT-LLM/examples/whisper/whisper_model_int8/decoder/* triton_models/whisper_decoder/1/"
      ],
      "metadata": {
        "id": "46krlGndR2sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/TensorRT-LLM/examples/whisper/triton_models/whisper_encoder/config.pbtxt\n",
        "\n",
        "\n",
        "name: \"whisper_encoder\"\n",
        "platform: \"tensorrt_plan\"\n",
        "max_batch_size: 8\n",
        "input [\n",
        "  {\n",
        "    name: \"input_features\"\n",
        "    data_type: TYPE_FP16\n",
        "    dims: [ 80, 3000 ]\n",
        "  }\n",
        "]\n",
        "output [\n",
        "  {\n",
        "    name: \"hidden_states\"\n",
        "    data_type: TYPE_FP16\n",
        "    dims: [ -1, 1280 ]\n",
        "  }\n",
        "]\n",
        "instance_group [\n",
        "  {\n",
        "    count: 1\n",
        "    kind: KIND_GPU\n",
        "    gpus: [ 0 ]\n",
        "  }\n",
        "]\n",
        "dynamic_batching {\n",
        "  preferred_batch_size: [ 1, 4, 8 ]\n",
        "  max_queue_delay_microseconds: 50000\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIWOt7cqTklm",
        "outputId": "7ea1c1d5-db07-4e90-ea53-5337ecc47216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/TensorRT-LLM/examples/whisper/triton_models/whisper_encoder/config.pbtxt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/TensorRT-LLM/examples/whisper/triton_models/whisper_decoder/config.pbtxt\n",
        "\n",
        "name: \"whisper_decoder\"\n",
        "platform: \"tensorrt_plan\"\n",
        "max_batch_size: 8\n",
        "input [\n",
        "  {\n",
        "    name: \"input_ids\"\n",
        "    data_type: TYPE_INT32\n",
        "    dims: [ -1 ]\n",
        "  },\n",
        "  {\n",
        "    name: \"encoder_output\"\n",
        "    data_type: TYPE_FP16\n",
        "    dims: [ -1, 1280 ]\n",
        "  }\n",
        "]\n",
        "output [\n",
        "  {\n",
        "    name: \"logits\"\n",
        "    data_type: TYPE_FP16\n",
        "    dims: [ -1, 51865 ]\n",
        "  }\n",
        "]\n",
        "instance_group [\n",
        "  {\n",
        "    count: 1\n",
        "    kind: KIND_GPU\n",
        "    gpus: [ 0 ]\n",
        "  }\n",
        "]\n",
        "dynamic_batching {\n",
        "  preferred_batch_size: [ 1, 4, 8 ]\n",
        "  max_queue_delay_microseconds: 50000\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAP3TFxhUNHP",
        "outputId": "85f46403-8e88-41f2-f74e-6883482b145f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/TensorRT-LLM/examples/whisper/triton_models/whisper_decoder/config.pbtxt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/TensorRT-LLM/examples/whisper/triton_models/whisper/config.pbtxt\n",
        "\n",
        "name: \"whisper\"\n",
        "platform: \"ensemble\"\n",
        "max_batch_size: 8\n",
        "input [\n",
        "  {\n",
        "    name: \"audio_features\"\n",
        "    data_type: TYPE_FP16\n",
        "    dims: [ 80, 3000 ]\n",
        "  },\n",
        "  {\n",
        "    name: \"decoder_input_ids\"\n",
        "    data_type: TYPE_INT32\n",
        "    dims: [ -1 ]\n",
        "  }\n",
        "]\n",
        "output [\n",
        "  {\n",
        "    name: \"output_logits\"\n",
        "    data_type: TYPE_FP16\n",
        "    dims: [ -1, 51865 ]\n",
        "  }\n",
        "]\n",
        "ensemble_scheduling {\n",
        "  step [\n",
        "    {\n",
        "      model_name: \"whisper_encoder\"\n",
        "      model_version: -1\n",
        "      input_map {\n",
        "        key: \"input_features\"\n",
        "        value: \"audio_features\"\n",
        "      }\n",
        "      output_map {\n",
        "        key: \"hidden_states\"\n",
        "        value: \"encoder_hidden_states\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      model_name: \"whisper_decoder\"\n",
        "      model_version: -1\n",
        "      input_map {\n",
        "        key: \"input_ids\"\n",
        "        value: \"decoder_input_ids\"\n",
        "      }\n",
        "      input_map {\n",
        "        key: \"encoder_output\"\n",
        "        value: \"encoder_hidden_states\"\n",
        "      }\n",
        "      output_map {\n",
        "        key: \"logits\"\n",
        "        value: \"output_logits\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpkkSh16Uart",
        "outputId": "3cbb7aa4-aae7-402a-f75b-f71a849bfa2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/TensorRT-LLM/examples/whisper/triton_models/whisper/config.pbtxt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "## 9. Conclusion\n",
        "\n",
        "This notebook has demonstrated the complete workflow for INT8 quantization of a Whisper model fine-tuned for Indian names recognition. We've successfully:\n",
        "\n",
        "1. Set up the required environment and dependencies\n",
        "2. Downloaded and converted a specialized Whisper model\n",
        "3. Applied INT8 weight-only quantization to reduce model size\n",
        "4. Built optimized TensorRT engines for inference\n",
        "5. Tested the model on sample audio\n",
        "6. Prepared the model for deployment on Triton Inference Server\n",
        "\n",
        "The quantized model provides several advantages over the original:\n",
        "- Approximately 75% reduction in model size\n",
        "- Improved inference speed (typically 2-4x faster)\n",
        "- Reduced memory requirements, enabling deployment on resource-constrained devices\n",
        "- Negligible loss in accuracy for speech recognition tasks\n",
        "\n",
        "This optimization enables efficient deployment of Whisper models in production environments, making accurate speech recognition more accessible and cost-effective. The INT8 quantized model is particularly valuable for scenarios requiring high-throughput processing of audio content, such as call centers, meeting transcription services, and accessibility tools.\n",
        "\n",
        "For further optimization, consider:\n",
        "- Experimenting with different quantization methods (symmetric vs asymmetric)\n",
        "- Fine-tuning the quantized model on domain-specific data\n",
        "- Implementing dynamic batching strategies for production deployment\n",
        "\n",
        "The combination of specialized fine-tuning (for Indian names) and quantization (for efficiency) represents a best-practice approach to deploying speech recognition models for real-world applications.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5lVdF7QVT9ei"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bXVQV22RT_Oh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}